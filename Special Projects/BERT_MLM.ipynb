{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-MLM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT8oDHQYqjlG"
      },
      "source": [
        "!pip install tf-nightly -q"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7a6YJ208vWg"
      },
      "source": [
        "import re\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from pprint import pprint\n",
        "from tensorflow import keras\n",
        "from dataclasses import dataclass\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNm8iz9Sm9N_"
      },
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    MAX_LEN = 256\n",
        "    BATCH_SIZE = 128\n",
        "    LR = 0.001\n",
        "    VOCAB_SIZE = 30000\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEAD = 8  \n",
        "    FF_DIM = 128  \n",
        "    NUM_LAYERS = 1\n",
        "\n",
        "\n",
        "config = Config()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3MYPEAvntSl"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDMaIG4Tnuna",
        "outputId": "308fa43f-a858-4145-a1e6-bfb1d21e8e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "\"\"\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\\n!tar -xf aclImdb_v1.tar.gz\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRhui29Vnwy8"
      },
      "source": [
        "def get_text_list_from_files(files):\n",
        "    text_list = []\n",
        "    for name in files:\n",
        "        with open(name) as f:\n",
        "            for line in f:\n",
        "                text_list.append(line)\n",
        "    return text_list\n",
        "\n",
        "\n",
        "def get_data_from_text_files(folder_name):\n",
        "\n",
        "    pos_files = glob.glob(\"aclImdb/\" + folder_name + \"/pos/*.txt\")\n",
        "    pos_texts = get_text_list_from_files(pos_files)\n",
        "    neg_files = glob.glob(\"aclImdb/\" + folder_name + \"/neg/*.txt\")\n",
        "    neg_texts = get_text_list_from_files(neg_files)\n",
        "    df = pd.DataFrame(\n",
        "        {\n",
        "            \"review\": pos_texts + neg_texts,\n",
        "            \"sentiment\": [0] * len(pos_texts) + [1] * len(neg_texts),\n",
        "        }\n",
        "    )\n",
        "    df = df.sample(len(df)).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = get_data_from_text_files(\"train\")\n",
        "test_df = get_data_from_text_files(\"test\")\n",
        "\n",
        "all_data = train_df.append(test_df)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnE4y1D6odV-"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kivgnh0AofNK"
      },
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "    return tf.strings.regex_replace(\n",
        "        stripped_html, \"[%s]\" % re.escape(\"!#$%&'()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
        "    )\n",
        "\n",
        "\n",
        "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
        "    \"\"\"Build Text vectorization layer\n",
        "\n",
        "    Args:\n",
        "      texts (list): List of string i.e input texts\n",
        "      vocab_size (int): vocab size\n",
        "      max_seq (int): Maximum sequence lenght.\n",
        "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
        "\n",
        "    Returns:\n",
        "        layers.Layer: Return TextVectorization Keras Layer\n",
        "    \"\"\"\n",
        "    vectorize_layer = TextVectorization(\n",
        "        max_tokens=vocab_size,\n",
        "        output_mode=\"int\",\n",
        "        standardize=custom_standardization,\n",
        "        output_sequence_length=max_seq,\n",
        "    )\n",
        "    vectorize_layer.adapt(texts)\n",
        "\n",
        "    # Insert mask token in vocabulary\n",
        "    vocab = vectorize_layer.get_vocabulary()\n",
        "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
        "    vectorize_layer.set_vocabulary(vocab)\n",
        "    return vectorize_layer\n",
        "\n",
        "\n",
        "vectorize_layer = get_vectorize_layer(\n",
        "    all_data.review.values.tolist(),\n",
        "    config.VOCAB_SIZE,\n",
        "    config.MAX_LEN,\n",
        "    special_tokens=[\"[mask]\"],\n",
        ")\n",
        "\n",
        "# Get mask token id for masked language model\n",
        "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]\n",
        "\n",
        "\n",
        "def encode(texts):\n",
        "    encoded_texts = vectorize_layer(texts)\n",
        "    return encoded_texts.numpy()\n",
        "\n",
        "\n",
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    # 15% BERT masking\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
        "    # Do not mask special tokens\n",
        "    inp_mask[encoded_texts <= 2] = False\n",
        "    # Set targets to -1 by default, it means ignore\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "    # Set labels for masked tokens\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "    # Prepare input\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\n",
        "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
        "    # This means leaving 10% unchanged\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "    encoded_texts_masked[\n",
        "        inp_mask_2mask\n",
        "    ] = mask_token_id  # mask token is the last in the dict\n",
        "\n",
        "    # Set 10% to a random token\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
        "        3, mask_token_id, inp_mask_2random.sum()\n",
        "    )\n",
        "\n",
        "    # Prepare sample_weights to pass to .fit() method\n",
        "    sample_weights = np.ones(labels.shape)\n",
        "    sample_weights[labels == -1] = 0\n",
        "\n",
        "    # y_labels would be same as encoded_texts i.e input tokens\n",
        "    y_labels = np.copy(encoded_texts)\n",
        "\n",
        "    return encoded_texts_masked, y_labels, sample_weights\n",
        "\n",
        "\n",
        "# We have 25000 examples for training\n",
        "x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n",
        "y_train = train_df.sentiment.values\n",
        "train_classifier_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(1000)\n",
        "    .batch(config.BATCH_SIZE)\n",
        ")\n",
        "\n",
        "# We have 25000 examples for testing\n",
        "x_test = encode(test_df.review.values)\n",
        "y_test = test_df.sentiment.values\n",
        "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
        "    config.BATCH_SIZE\n",
        ")\n",
        "\n",
        "# Build dataset for end to end model input (will be used at the end)\n",
        "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (test_df.review.values, y_test)\n",
        ").batch(config.BATCH_SIZE)\n",
        "\n",
        "# Prepare data for masked language model\n",
        "x_all_review = encode(all_data.review.values)\n",
        "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
        "    x_all_review\n",
        ")\n",
        "\n",
        "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_masked_train, y_masked_labels, sample_weights)\n",
        ")\n",
        "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoAE3OIkpFZo"
      },
      "source": [
        "### BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ggk359fpGMm",
        "outputId": "b0cc28f3-eb2c-4559-aa90-2eefb424f2c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "def bert_module(query, key, value, i):\n",
        "    # Multi headed self-attention\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=config.NUM_HEAD,\n",
        "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
        "        name=\"encoder_{}/multiheadattention\".format(i),\n",
        "    )(query, key, value)\n",
        "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
        "        attention_output\n",
        "    )\n",
        "    attention_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
        "    )(query + attention_output)\n",
        "\n",
        "    # Feed-forward layer\n",
        "    ffn = keras.Sequential(\n",
        "        [\n",
        "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
        "            layers.Dense(config.EMBED_DIM),\n",
        "        ],\n",
        "        name=\"encoder_{}/ffn\".format(i),\n",
        "    )\n",
        "    ffn_output = ffn(attention_output)\n",
        "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
        "        ffn_output\n",
        "    )\n",
        "    sequence_output = layers.LayerNormalization(\n",
        "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
        "    )(attention_output + ffn_output)\n",
        "    return sequence_output\n",
        "\n",
        "\n",
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
        "            if pos != 0\n",
        "            else np.zeros(d_emb)\n",
        "            for pos in range(max_len)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc\n",
        "\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "    reduction=tf.keras.losses.Reduction.NONE\n",
        ")\n",
        "loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "\n",
        "class MaskedLanguageModel(tf.keras.Model):\n",
        "    def train_step(self, inputs):\n",
        "        if len(inputs) == 3:\n",
        "            features, labels, sample_weight = inputs\n",
        "        else:\n",
        "            features, labels = inputs\n",
        "            sample_weight = None\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(features, training=True)\n",
        "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Compute our own metrics\n",
        "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # We list our `Metric` objects here so that `reset_states()` can be\n",
        "        # called automatically at the start of each epoch\n",
        "        # or at the start of `evaluate()`.\n",
        "        # If you don't implement this property, you have to call\n",
        "        # `reset_states()` yourself at the time of your choosing.\n",
        "        return [loss_tracker]\n",
        "\n",
        "\n",
        "def create_masked_language_bert_model():\n",
        "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
        "\n",
        "    word_embeddings = layers.Embedding(\n",
        "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
        "    )(inputs)\n",
        "    position_embeddings = layers.Embedding(\n",
        "        input_dim=config.MAX_LEN,\n",
        "        output_dim=config.EMBED_DIM,\n",
        "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
        "        name=\"position_embedding\",\n",
        "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
        "    embeddings = word_embeddings + position_embeddings\n",
        "\n",
        "    encoder_output = embeddings\n",
        "    for i in range(config.NUM_LAYERS):\n",
        "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
        "\n",
        "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
        "        encoder_output\n",
        "    )\n",
        "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
        "    mlm_model.compile(optimizer=optimizer)\n",
        "    return mlm_model\n",
        "\n",
        "\n",
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "token2id = {y: x for x, y in id2token.items()}\n",
        "\n",
        "\n",
        "class MaskedTextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self, sample_tokens, top_k=5):\n",
        "        self.sample_tokens = sample_tokens\n",
        "        self.k = top_k\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
        "\n",
        "    def convert_ids_to_tokens(self, id):\n",
        "        return id2token[id]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        prediction = self.model.predict(self.sample_tokens)\n",
        "\n",
        "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
        "        masked_index = masked_index[1]\n",
        "        mask_prediction = prediction[0][masked_index]\n",
        "\n",
        "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
        "        values = mask_prediction[0][top_indices]\n",
        "\n",
        "        for i in range(len(top_indices)):\n",
        "            p = top_indices[i]\n",
        "            v = values[i]\n",
        "            tokens = np.copy(sample_tokens[0])\n",
        "            tokens[masked_index[0]] = p\n",
        "            result = {\n",
        "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
        "                \"prediction\": self.decode(tokens),\n",
        "                \"probability\": v,\n",
        "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
        "            }\n",
        "            pprint(result)\n",
        "\n",
        "\n",
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "generator_callback = MaskedTextGenerator(sample_tokens.numpy())\n",
        "\n",
        "bert_masked_model = create_masked_language_bert_model()\n",
        "bert_masked_model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"masked_bert_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 256)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 256, 128)     3840000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add (TFOpLambd (None, 256, 128)     0           word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/multiheadattention (M (None, 256, 128)     66048       tf.__operators__.add[0][0]       \n",
            "                                                                 tf.__operators__.add[0][0]       \n",
            "                                                                 tf.__operators__.add[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/att_dropout (Dropout) (None, 256, 128)     0           encoder_0/multiheadattention[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_1 (TFOpLam (None, 256, 128)     0           tf.__operators__.add[0][0]       \n",
            "                                                                 encoder_0/att_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn (Sequential)      (None, 256, 128)     33024       encoder_0/att_layernormalization[\n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_0/ffn[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_2 (TFOpLam (None, 256, 128)     0           encoder_0/att_layernormalization[\n",
            "                                                                 encoder_0/ffn_dropout[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_0/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_2[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mlm_cls (Dense)                 (None, 256, 30000)   3870000     encoder_0/ffn_layernormalization[\n",
            "==================================================================================================\n",
            "Total params: 7,809,584\n",
            "Trainable params: 7,809,584\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2fjXJJ0rNp5",
        "outputId": "8f256f49-f526-44a8-c7c9-0f599b1f9a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "bert_masked_model.fit(mlm_ds, epochs=1, callbacks=[generator_callback])\n",
        "bert_masked_model.save(\"bert_mlm_imdb.h5\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 11049s 28s/step - loss: 7.2207\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'this',\n",
            " 'prediction': 'i have watched this this and it was awesome',\n",
            " 'probability': 0.04847152}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'a',\n",
            " 'prediction': 'i have watched this a and it was awesome',\n",
            " 'probability': 0.043183763}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'i',\n",
            " 'prediction': 'i have watched this i and it was awesome',\n",
            " 'probability': 0.041392293}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'of',\n",
            " 'prediction': 'i have watched this of and it was awesome',\n",
            " 'probability': 0.03380835}\n",
            "{'input_text': 'i have watched this [mask] and it was awesome',\n",
            " 'predicted mask token': 'is',\n",
            " 'prediction': 'i have watched this is and it was awesome',\n",
            " 'probability': 0.033406045}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM04Vik7rRC4"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}