{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence-to-Sequence NMT with Attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQiFsN9H8TPm"
      },
      "source": [
        "### TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz50LCTPt-gj",
        "outputId": "bfd4f8a7-45de-4328-892d-f5faad80f387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!wget --quiet http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDOCnDN_9MLh",
        "outputId": "c2767c06-bc98-4763-924f-4e8c236bead1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s69Ysvj8gXD"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "import string\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from pickle import load, dump\n",
        "from typing import List, Tuple\n",
        "from unicodedata import normalize\n",
        "from keras.models import load_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZYJZP-y9TiH"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7jLbvh-9VbB"
      },
      "source": [
        "# Start of sentence\n",
        "SOS = \"<start>\"\n",
        "# End of sentence\n",
        "EOS = \"<end>\"\n",
        "# Relevant punctuation\n",
        "PUNCTUATION = set(\"?,!.\")\n",
        "\n",
        "\n",
        "def load_dataset(filename: str) -> str:\n",
        "    \"\"\"\n",
        "    load dataset into memory\n",
        "    \"\"\"\n",
        "    with open(filename, mode = \"rt\", encoding = \"utf-8\") as fp:\n",
        "        return fp.read()\n",
        "\n",
        "\n",
        "def to_pairs(dataset: str, limit: int = None, shuffle = False) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Split dataset into pairs of sentences, discards dataset line info.\n",
        "\n",
        "    e.g.\n",
        "    input -> 'Go.\\tGeh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org\n",
        "    #2877272 (CM) & #8597805 (Roujin)'\n",
        "    output -> [('Go.', 'Geh.')]\n",
        "\n",
        "    :param dataset: dataset containing examples of translations between\n",
        "    two languages\n",
        "    the examples are delimited by `\\n` and the contents of the lines are\n",
        "    delimited by `\\t`\n",
        "    :param limit: number that limit dataset size (optional)\n",
        "    :param shuffle: default is True\n",
        "    :return: list of pairs\n",
        "    \"\"\"\n",
        "    assert isinstance(limit, (int, type(None))), TypeError(\n",
        "        \"the limit value must be an integer\"\n",
        "    )\n",
        "    lines = dataset.strip().split(\"\\n\")\n",
        "    # Radom dataset\n",
        "    if shuffle is True:\n",
        "        random.shuffle(lines)\n",
        "    number_examples = limit or len(lines)  # if None get all\n",
        "    pairs = []\n",
        "    for line in lines[: abs(number_examples)]:\n",
        "        # take only source and target\n",
        "        src, trg, _ = line.split(\"\\t\")\n",
        "        pairs.append((src, trg))\n",
        "\n",
        "    # dataset size check\n",
        "    assert len(pairs) == number_examples\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def separe_punctuation(token: str) -> str:\n",
        "    \"\"\"\n",
        "    Separe punctuation if exists\n",
        "    \"\"\"\n",
        "\n",
        "    if not set(token).intersection(PUNCTUATION):\n",
        "        return token\n",
        "    for p in PUNCTUATION:\n",
        "        token = f\" {p} \".join(token.split(p))\n",
        "    return \" \".join(token.split())\n",
        "\n",
        "\n",
        "def preprocess(sentence: str, add_start_end: bool=True) -> str:\n",
        "    \"\"\"\n",
        "\n",
        "    - convert lowercase\n",
        "    - remove numbers\n",
        "    - remove special characters\n",
        "    - separe punctuation\n",
        "    - add start-of-sentence <start> and end-of-sentence <end>\n",
        "\n",
        "    :param add_start_end: add SOS (start-of-sentence) and EOS (end-of-sentence)\n",
        "    \"\"\"\n",
        "    re_print = re.compile(f\"[^{re.escape(string.printable)}]\")\n",
        "    # convert lowercase and normalizing unicode characters\n",
        "    sentence = (\n",
        "        normalize(\"NFD\", sentence.lower()).encode(\"ascii\", \"ignore\").decode(\"UTF-8\")\n",
        "    )\n",
        "    cleaned_tokens = []\n",
        "    # tokenize sentence on white space\n",
        "    for token in sentence.split():\n",
        "        # removing non-printable chars form each token\n",
        "        token = re_print.sub(\"\", token).strip()\n",
        "        # ignore tokens with numbers\n",
        "        if re.findall(\"[0-9]\", token):\n",
        "            continue\n",
        "        # add space between words and punctuation eg: \"ok?go!\" => \"ok ? go !\"\n",
        "        token = separe_punctuation(token)\n",
        "        cleaned_tokens.append(token)\n",
        "\n",
        "    # rebuild sentence with space between tokens\n",
        "    sentence = \" \".join(cleaned_tokens)\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    if add_start_end is True:\n",
        "        sentence = f\"{SOS} {sentence} {EOS}\"\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def dataset_preprocess(dataset: List[Tuple[str, str]]) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Returns processed database\n",
        "\n",
        "    :param dataset: list of sentence pairs\n",
        "    :return: list of paralel data e.g. \n",
        "    (['first source sentence', 'second', ...], ['first target sentence', 'second', ...])\n",
        "    \"\"\"\n",
        "    source_cleaned = []\n",
        "    target_cleaned = []\n",
        "    for source, target in dataset:\n",
        "        source_cleaned.append(preprocess(source))\n",
        "        target_cleaned.append(preprocess(target))\n",
        "    return source_cleaned, target_cleaned"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tucoha8s-jlT"
      },
      "source": [
        "### Create Dataset\n",
        "- limit number of examples\n",
        "- load dataset into pairs ```[('Be nice.', 'Seien Sie nett!'), ('Beat it.', 'Geh weg!'), ...]```\n",
        "- preprocessing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2HsFaE1-z9l",
        "outputId": "c44798f5-a06f-424c-f9f2-99856dcc4f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "NUM_EXAMPLES = 10000 \n",
        "\n",
        "filename = 'deu.txt'\n",
        "dataset = load_dataset(filename)\n",
        "\n",
        "pairs = to_pairs(dataset, limit = NUM_EXAMPLES)\n",
        "print(f\"Dataset size: {len(pairs)}\")\n",
        "raw_data_en, raw_data_ge = dataset_preprocess(pairs)\n",
        "\n",
        "# show last 5 pairs\n",
        "for pair in zip(raw_data_en[-5:],raw_data_ge[-5:]):\n",
        "    print(pair)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size: 10000\n",
            "('<start> tom was crying . <end>', '<start> tom flennte . <end>')\n",
            "('<start> tom was eating . <end>', '<start> tom hat gegessen . <end>')\n",
            "('<start> tom was famous . <end>', '<start> tom war beruhmt . <end>')\n",
            "('<start> tom was framed . <end>', '<start> tom wurde reingelegt . <end>')\n",
            "('<start> tom was fuming . <end>', '<start> tom war wutend . <end>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahGQNAxI_ELG"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0avgYRfJ_Ezs"
      },
      "source": [
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = '')\n",
        "en_tokenizer.fit_on_texts(raw_data_en)\n",
        "\n",
        "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,padding = 'post')\n",
        "\n",
        "ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = '')\n",
        "ge_tokenizer.fit_on_texts(raw_data_ge)\n",
        "\n",
        "data_ge = ge_tokenizer.texts_to_sequences(raw_data_ge)\n",
        "data_ge = tf.keras.preprocessing.sequence.pad_sequences(data_ge,padding = 'post')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyJqYeIg_Nh-"
      },
      "source": [
        "def max_len(tensor):\n",
        "    return max( len(t) for t in tensor)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk8ULJJy_aGb"
      },
      "source": [
        "### Training Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxtLh27G_cyk",
        "outputId": "3d5144c2-cd54-41ab-84db-1534db60c016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train,  X_test, Y_train, Y_test = train_test_split(data_en,data_ge,test_size = 0.2)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(X_train)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "rnn_units = 1024\n",
        "dense_units = 1024\n",
        "Dtype = tf.float32   #used to initialize DecoderCell Zero state\n",
        "\n",
        "Tx = max_len(data_en)\n",
        "Ty = max_len(data_ge)  \n",
        "\n",
        "input_vocab_size = len(en_tokenizer.word_index)+1  \n",
        "output_vocab_size = len(ge_tokenizer.word_index)+ 1\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 9)\n",
            "(64, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHly9de8_yUW"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOFgUIkb_0Di"
      },
      "source": [
        "# ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim = input_vocab_size,\n",
        "                                                           output_dim = embedding_dims)\n",
        "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences = True, \n",
        "                                                     return_state = True )\n",
        "    \n",
        "# DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim = output_vocab_size,\n",
        "                                                           output_dim = embedding_dims) \n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler = self.sampler,\n",
        "                                                output_layer = self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
        "                                          memory_sequence_length = memory_sequence_length)\n",
        "\n",
        "    # wrap decodernn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state, Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state = encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrllG81nA4GH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}