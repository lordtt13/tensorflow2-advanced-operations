{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sequence-to-Sequence NMT with Attention.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQiFsN9H8TPm"
      },
      "source": [
        "### TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz50LCTPt-gj",
        "outputId": "bfd4f8a7-45de-4328-892d-f5faad80f387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!wget --quiet http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDOCnDN_9MLh",
        "outputId": "c2767c06-bc98-4763-924f-4e8c236bead1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s69Ysvj8gXD"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "import string\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from pickle import load, dump\n",
        "from typing import List, Tuple\n",
        "from unicodedata import normalize\n",
        "from keras.models import load_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZYJZP-y9TiH"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7jLbvh-9VbB"
      },
      "source": [
        "# Start of sentence\n",
        "SOS = \"<start>\"\n",
        "# End of sentence\n",
        "EOS = \"<end>\"\n",
        "# Relevant punctuation\n",
        "PUNCTUATION = set(\"?,!.\")\n",
        "\n",
        "\n",
        "def load_dataset(filename: str) -> str:\n",
        "    \"\"\"\n",
        "    load dataset into memory\n",
        "    \"\"\"\n",
        "    with open(filename, mode = \"rt\", encoding = \"utf-8\") as fp:\n",
        "        return fp.read()\n",
        "\n",
        "\n",
        "def to_pairs(dataset: str, limit: int = None, shuffle = False) -> List[Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Split dataset into pairs of sentences, discards dataset line info.\n",
        "\n",
        "    e.g.\n",
        "    input -> 'Go.\\tGeh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org\n",
        "    #2877272 (CM) & #8597805 (Roujin)'\n",
        "    output -> [('Go.', 'Geh.')]\n",
        "\n",
        "    :param dataset: dataset containing examples of translations between\n",
        "    two languages\n",
        "    the examples are delimited by `\\n` and the contents of the lines are\n",
        "    delimited by `\\t`\n",
        "    :param limit: number that limit dataset size (optional)\n",
        "    :param shuffle: default is True\n",
        "    :return: list of pairs\n",
        "    \"\"\"\n",
        "    assert isinstance(limit, (int, type(None))), TypeError(\n",
        "        \"the limit value must be an integer\"\n",
        "    )\n",
        "    lines = dataset.strip().split(\"\\n\")\n",
        "    # Radom dataset\n",
        "    if shuffle is True:\n",
        "        random.shuffle(lines)\n",
        "    number_examples = limit or len(lines)  # if None get all\n",
        "    pairs = []\n",
        "    for line in lines[: abs(number_examples)]:\n",
        "        # take only source and target\n",
        "        src, trg, _ = line.split(\"\\t\")\n",
        "        pairs.append((src, trg))\n",
        "\n",
        "    # dataset size check\n",
        "    assert len(pairs) == number_examples\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def separe_punctuation(token: str) -> str:\n",
        "    \"\"\"\n",
        "    Separe punctuation if exists\n",
        "    \"\"\"\n",
        "\n",
        "    if not set(token).intersection(PUNCTUATION):\n",
        "        return token\n",
        "    for p in PUNCTUATION:\n",
        "        token = f\" {p} \".join(token.split(p))\n",
        "    return \" \".join(token.split())\n",
        "\n",
        "\n",
        "def preprocess(sentence: str, add_start_end: bool=True) -> str:\n",
        "    \"\"\"\n",
        "\n",
        "    - convert lowercase\n",
        "    - remove numbers\n",
        "    - remove special characters\n",
        "    - separe punctuation\n",
        "    - add start-of-sentence <start> and end-of-sentence <end>\n",
        "\n",
        "    :param add_start_end: add SOS (start-of-sentence) and EOS (end-of-sentence)\n",
        "    \"\"\"\n",
        "    re_print = re.compile(f\"[^{re.escape(string.printable)}]\")\n",
        "    # convert lowercase and normalizing unicode characters\n",
        "    sentence = (\n",
        "        normalize(\"NFD\", sentence.lower()).encode(\"ascii\", \"ignore\").decode(\"UTF-8\")\n",
        "    )\n",
        "    cleaned_tokens = []\n",
        "    # tokenize sentence on white space\n",
        "    for token in sentence.split():\n",
        "        # removing non-printable chars form each token\n",
        "        token = re_print.sub(\"\", token).strip()\n",
        "        # ignore tokens with numbers\n",
        "        if re.findall(\"[0-9]\", token):\n",
        "            continue\n",
        "        # add space between words and punctuation eg: \"ok?go!\" => \"ok ? go !\"\n",
        "        token = separe_punctuation(token)\n",
        "        cleaned_tokens.append(token)\n",
        "\n",
        "    # rebuild sentence with space between tokens\n",
        "    sentence = \" \".join(cleaned_tokens)\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    if add_start_end is True:\n",
        "        sentence = f\"{SOS} {sentence} {EOS}\"\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def dataset_preprocess(dataset: List[Tuple[str, str]]) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Returns processed database\n",
        "\n",
        "    :param dataset: list of sentence pairs\n",
        "    :return: list of paralel data e.g. \n",
        "    (['first source sentence', 'second', ...], ['first target sentence', 'second', ...])\n",
        "    \"\"\"\n",
        "    source_cleaned = []\n",
        "    target_cleaned = []\n",
        "    for source, target in dataset:\n",
        "        source_cleaned.append(preprocess(source))\n",
        "        target_cleaned.append(preprocess(target))\n",
        "    return source_cleaned, target_cleaned"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tucoha8s-jlT"
      },
      "source": [
        "### Create Dataset\n",
        "- limit number of examples\n",
        "- load dataset into pairs ```[('Be nice.', 'Seien Sie nett!'), ('Beat it.', 'Geh weg!'), ...]```\n",
        "- preprocessing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2HsFaE1-z9l",
        "outputId": "c44798f5-a06f-424c-f9f2-99856dcc4f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "NUM_EXAMPLES = 10000 \n",
        "\n",
        "filename = 'deu.txt'\n",
        "dataset = load_dataset(filename)\n",
        "\n",
        "pairs = to_pairs(dataset, limit = NUM_EXAMPLES)\n",
        "print(f\"Dataset size: {len(pairs)}\")\n",
        "raw_data_en, raw_data_ge = dataset_preprocess(pairs)\n",
        "\n",
        "# show last 5 pairs\n",
        "for pair in zip(raw_data_en[-5:],raw_data_ge[-5:]):\n",
        "    print(pair)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size: 10000\n",
            "('<start> tom was crying . <end>', '<start> tom flennte . <end>')\n",
            "('<start> tom was eating . <end>', '<start> tom hat gegessen . <end>')\n",
            "('<start> tom was famous . <end>', '<start> tom war beruhmt . <end>')\n",
            "('<start> tom was framed . <end>', '<start> tom wurde reingelegt . <end>')\n",
            "('<start> tom was fuming . <end>', '<start> tom war wutend . <end>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahGQNAxI_ELG"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0avgYRfJ_Ezs"
      },
      "source": [
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = '')\n",
        "en_tokenizer.fit_on_texts(raw_data_en)\n",
        "\n",
        "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,padding = 'post')\n",
        "\n",
        "ge_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters = '')\n",
        "ge_tokenizer.fit_on_texts(raw_data_ge)\n",
        "\n",
        "data_ge = ge_tokenizer.texts_to_sequences(raw_data_ge)\n",
        "data_ge = tf.keras.preprocessing.sequence.pad_sequences(data_ge,padding = 'post')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyJqYeIg_Nh-"
      },
      "source": [
        "def max_len(tensor):\n",
        "    return max( len(t) for t in tensor)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk8ULJJy_aGb"
      },
      "source": [
        "### Training Prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxtLh27G_cyk",
        "outputId": "3d5144c2-cd54-41ab-84db-1534db60c016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train,  X_test, Y_train, Y_test = train_test_split(data_en,data_ge,test_size = 0.2)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(X_train)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "rnn_units = 1024\n",
        "dense_units = 1024\n",
        "Dtype = tf.float32   #used to initialize DecoderCell Zero state\n",
        "\n",
        "Tx = max_len(data_en)\n",
        "Ty = max_len(data_ge)  \n",
        "\n",
        "input_vocab_size = len(en_tokenizer.word_index)+1  \n",
        "output_vocab_size = len(ge_tokenizer.word_index)+ 1\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True)\n",
        "example_X, example_Y = next(iter(dataset))\n",
        "print(example_X.shape) \n",
        "print(example_Y.shape) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 9)\n",
            "(64, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHly9de8_yUW"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOFgUIkb_0Di"
      },
      "source": [
        "# ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim = input_vocab_size,\n",
        "                                                           output_dim = embedding_dims)\n",
        "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences = True, \n",
        "                                                     return_state = True )\n",
        "    \n",
        "# DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim = output_vocab_size,\n",
        "                                                           output_dim = embedding_dims) \n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler = self.sampler,\n",
        "                                                output_layer = self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
        "                                          memory_sequence_length = memory_sequence_length)\n",
        "\n",
        "    # wrap decodernn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state, Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state = encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knBOsQSnA6U5"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGogJtuwA7T2"
      },
      "source": [
        "def loss_function(y_pred, y):\n",
        "   \n",
        "    #shape of y [batch_size, ty]\n",
        "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True,\n",
        "                                                                                  reduction = 'none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true = y, y_pred = y_pred)\n",
        "    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype = loss.dtype)\n",
        "    loss = mask*loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n",
        "\n",
        "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state = encoder_initial_cell_state)\n",
        "\n",
        "        # [last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        \n",
        "         \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        # compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] # ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        # Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state = [a_tx, c_tx],\n",
        "                                                                           Dtype = tf.float32)\n",
        "        \n",
        "        # BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state = decoder_initial_state,\n",
        "                                               sequence_length = BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        # Calculate loss\n",
        "\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    # Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    # grads_and_vars â€“ List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0UdqDXrC0Nh"
      },
      "source": [
        "# RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE8wyA9kC2as",
        "outputId": "b0e50eef-77bb-4bc6-b7bd-5e5d75e3be97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 15\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%5 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 3.3131473064422607 epoch 1 batch 5 \n",
            "total loss: 2.262629747390747 epoch 1 batch 10 \n",
            "total loss: 2.076655149459839 epoch 1 batch 15 \n",
            "total loss: 2.0565059185028076 epoch 1 batch 20 \n",
            "total loss: 1.9498294591903687 epoch 1 batch 25 \n",
            "total loss: 2.079404830932617 epoch 1 batch 30 \n",
            "total loss: 1.9453245401382446 epoch 1 batch 35 \n",
            "total loss: 1.8376885652542114 epoch 1 batch 40 \n",
            "total loss: 1.7034138441085815 epoch 1 batch 45 \n",
            "total loss: 1.6252403259277344 epoch 1 batch 50 \n",
            "total loss: 1.6810463666915894 epoch 1 batch 55 \n",
            "total loss: 1.746016502380371 epoch 1 batch 60 \n",
            "total loss: 1.541724681854248 epoch 1 batch 65 \n",
            "total loss: 1.7413946390151978 epoch 1 batch 70 \n",
            "total loss: 1.6177066564559937 epoch 1 batch 75 \n",
            "total loss: 1.5472936630249023 epoch 1 batch 80 \n",
            "total loss: 1.570003628730774 epoch 1 batch 85 \n",
            "total loss: 1.6743879318237305 epoch 1 batch 90 \n",
            "total loss: 1.6717625856399536 epoch 1 batch 95 \n",
            "total loss: 1.668370246887207 epoch 1 batch 100 \n",
            "total loss: 1.5403496026992798 epoch 1 batch 105 \n",
            "total loss: 1.4073272943496704 epoch 1 batch 110 \n",
            "total loss: 1.460111141204834 epoch 1 batch 115 \n",
            "total loss: 1.4983348846435547 epoch 1 batch 120 \n",
            "total loss: 1.5121402740478516 epoch 1 batch 125 \n",
            "total loss: 1.4209213256835938 epoch 2 batch 5 \n",
            "total loss: 1.3460760116577148 epoch 2 batch 10 \n",
            "total loss: 1.3592233657836914 epoch 2 batch 15 \n",
            "total loss: 1.3675919771194458 epoch 2 batch 20 \n",
            "total loss: 1.3779386281967163 epoch 2 batch 25 \n",
            "total loss: 1.2639354467391968 epoch 2 batch 30 \n",
            "total loss: 1.4173202514648438 epoch 2 batch 35 \n",
            "total loss: 1.3066202402114868 epoch 2 batch 40 \n",
            "total loss: 1.3144572973251343 epoch 2 batch 45 \n",
            "total loss: 1.3124245405197144 epoch 2 batch 50 \n",
            "total loss: 1.2415897846221924 epoch 2 batch 55 \n",
            "total loss: 1.3457554578781128 epoch 2 batch 60 \n",
            "total loss: 1.258589267730713 epoch 2 batch 65 \n",
            "total loss: 1.2668031454086304 epoch 2 batch 70 \n",
            "total loss: 1.2151991128921509 epoch 2 batch 75 \n",
            "total loss: 1.2666651010513306 epoch 2 batch 80 \n",
            "total loss: 1.2504159212112427 epoch 2 batch 85 \n",
            "total loss: 1.3770875930786133 epoch 2 batch 90 \n",
            "total loss: 1.3025121688842773 epoch 2 batch 95 \n",
            "total loss: 1.1424115896224976 epoch 2 batch 100 \n",
            "total loss: 1.345348834991455 epoch 2 batch 105 \n",
            "total loss: 1.279067873954773 epoch 2 batch 110 \n",
            "total loss: 1.3861055374145508 epoch 2 batch 115 \n",
            "total loss: 1.213171124458313 epoch 2 batch 120 \n",
            "total loss: 1.2697309255599976 epoch 2 batch 125 \n",
            "total loss: 1.1639914512634277 epoch 3 batch 5 \n",
            "total loss: 1.0508875846862793 epoch 3 batch 10 \n",
            "total loss: 1.2613621950149536 epoch 3 batch 15 \n",
            "total loss: 1.088398814201355 epoch 3 batch 20 \n",
            "total loss: 1.0650981664657593 epoch 3 batch 25 \n",
            "total loss: 1.2212893962860107 epoch 3 batch 30 \n",
            "total loss: 1.2027665376663208 epoch 3 batch 35 \n",
            "total loss: 1.0655021667480469 epoch 3 batch 40 \n",
            "total loss: 1.1271189451217651 epoch 3 batch 45 \n",
            "total loss: 1.278848648071289 epoch 3 batch 50 \n",
            "total loss: 1.2237088680267334 epoch 3 batch 55 \n",
            "total loss: 1.1380597352981567 epoch 3 batch 60 \n",
            "total loss: 1.2329784631729126 epoch 3 batch 65 \n",
            "total loss: 1.1679648160934448 epoch 3 batch 70 \n",
            "total loss: 1.1478290557861328 epoch 3 batch 75 \n",
            "total loss: 1.1224186420440674 epoch 3 batch 80 \n",
            "total loss: 1.19285249710083 epoch 3 batch 85 \n",
            "total loss: 1.119552493095398 epoch 3 batch 90 \n",
            "total loss: 1.0287045240402222 epoch 3 batch 95 \n",
            "total loss: 1.207544207572937 epoch 3 batch 100 \n",
            "total loss: 1.15071439743042 epoch 3 batch 105 \n",
            "total loss: 1.0207246541976929 epoch 3 batch 110 \n",
            "total loss: 1.106168508529663 epoch 3 batch 115 \n",
            "total loss: 1.179199457168579 epoch 3 batch 120 \n",
            "total loss: 1.0894166231155396 epoch 3 batch 125 \n",
            "total loss: 1.0413081645965576 epoch 4 batch 5 \n",
            "total loss: 0.9210966229438782 epoch 4 batch 10 \n",
            "total loss: 0.9850296974182129 epoch 4 batch 15 \n",
            "total loss: 1.014680027961731 epoch 4 batch 20 \n",
            "total loss: 0.93475741147995 epoch 4 batch 25 \n",
            "total loss: 1.049116611480713 epoch 4 batch 30 \n",
            "total loss: 0.9272721409797668 epoch 4 batch 35 \n",
            "total loss: 0.9593403339385986 epoch 4 batch 40 \n",
            "total loss: 1.0266751050949097 epoch 4 batch 45 \n",
            "total loss: 0.9747779965400696 epoch 4 batch 50 \n",
            "total loss: 0.9145916104316711 epoch 4 batch 55 \n",
            "total loss: 0.8365026116371155 epoch 4 batch 60 \n",
            "total loss: 1.0202082395553589 epoch 4 batch 65 \n",
            "total loss: 0.8598327040672302 epoch 4 batch 70 \n",
            "total loss: 1.0325522422790527 epoch 4 batch 75 \n",
            "total loss: 0.9515201449394226 epoch 4 batch 80 \n",
            "total loss: 0.9964126944541931 epoch 4 batch 85 \n",
            "total loss: 0.9897280335426331 epoch 4 batch 90 \n",
            "total loss: 1.0010203123092651 epoch 4 batch 95 \n",
            "total loss: 0.8650502562522888 epoch 4 batch 100 \n",
            "total loss: 0.9365186095237732 epoch 4 batch 105 \n",
            "total loss: 1.0929991006851196 epoch 4 batch 110 \n",
            "total loss: 0.945382833480835 epoch 4 batch 115 \n",
            "total loss: 0.9508476853370667 epoch 4 batch 120 \n",
            "total loss: 0.9718138575553894 epoch 4 batch 125 \n",
            "total loss: 0.8324757218360901 epoch 5 batch 5 \n",
            "total loss: 0.7844789028167725 epoch 5 batch 10 \n",
            "total loss: 0.8106510043144226 epoch 5 batch 15 \n",
            "total loss: 0.803570568561554 epoch 5 batch 20 \n",
            "total loss: 0.8090154528617859 epoch 5 batch 25 \n",
            "total loss: 0.8195126056671143 epoch 5 batch 30 \n",
            "total loss: 0.8036940693855286 epoch 5 batch 35 \n",
            "total loss: 0.7900392413139343 epoch 5 batch 40 \n",
            "total loss: 0.8424350619316101 epoch 5 batch 45 \n",
            "total loss: 0.8752734065055847 epoch 5 batch 50 \n",
            "total loss: 0.8561210632324219 epoch 5 batch 55 \n",
            "total loss: 0.8378472924232483 epoch 5 batch 60 \n",
            "total loss: 0.8049535155296326 epoch 5 batch 65 \n",
            "total loss: 0.8142130970954895 epoch 5 batch 70 \n",
            "total loss: 0.8726617693901062 epoch 5 batch 75 \n",
            "total loss: 0.8320448398590088 epoch 5 batch 80 \n",
            "total loss: 0.8489913940429688 epoch 5 batch 85 \n",
            "total loss: 0.8406016826629639 epoch 5 batch 90 \n",
            "total loss: 0.9065437912940979 epoch 5 batch 95 \n",
            "total loss: 0.8197242617607117 epoch 5 batch 100 \n",
            "total loss: 0.7268983721733093 epoch 5 batch 105 \n",
            "total loss: 0.7865772843360901 epoch 5 batch 110 \n",
            "total loss: 0.7648763060569763 epoch 5 batch 115 \n",
            "total loss: 0.7943487763404846 epoch 5 batch 120 \n",
            "total loss: 0.8250994682312012 epoch 5 batch 125 \n",
            "total loss: 0.683687150478363 epoch 6 batch 5 \n",
            "total loss: 0.5904147028923035 epoch 6 batch 10 \n",
            "total loss: 0.7031916975975037 epoch 6 batch 15 \n",
            "total loss: 0.7269490361213684 epoch 6 batch 20 \n",
            "total loss: 0.6760105490684509 epoch 6 batch 25 \n",
            "total loss: 0.6332896947860718 epoch 6 batch 30 \n",
            "total loss: 0.6815438866615295 epoch 6 batch 35 \n",
            "total loss: 0.7260259985923767 epoch 6 batch 40 \n",
            "total loss: 0.7215139865875244 epoch 6 batch 45 \n",
            "total loss: 0.7402538657188416 epoch 6 batch 50 \n",
            "total loss: 0.6799746155738831 epoch 6 batch 55 \n",
            "total loss: 0.7053351998329163 epoch 6 batch 60 \n",
            "total loss: 0.6538701057434082 epoch 6 batch 65 \n",
            "total loss: 0.6056293845176697 epoch 6 batch 70 \n",
            "total loss: 0.6062895655632019 epoch 6 batch 75 \n",
            "total loss: 0.7231450080871582 epoch 6 batch 80 \n",
            "total loss: 0.681807816028595 epoch 6 batch 85 \n",
            "total loss: 0.6462363600730896 epoch 6 batch 90 \n",
            "total loss: 0.7676427960395813 epoch 6 batch 95 \n",
            "total loss: 0.7020360827445984 epoch 6 batch 100 \n",
            "total loss: 0.7536256909370422 epoch 6 batch 105 \n",
            "total loss: 0.7068844437599182 epoch 6 batch 110 \n",
            "total loss: 0.7053210735321045 epoch 6 batch 115 \n",
            "total loss: 0.6736046671867371 epoch 6 batch 120 \n",
            "total loss: 0.7102837562561035 epoch 6 batch 125 \n",
            "total loss: 0.5404660701751709 epoch 7 batch 5 \n",
            "total loss: 0.5408680438995361 epoch 7 batch 10 \n",
            "total loss: 0.4557662308216095 epoch 7 batch 15 \n",
            "total loss: 0.5549368262290955 epoch 7 batch 20 \n",
            "total loss: 0.5410239696502686 epoch 7 batch 25 \n",
            "total loss: 0.5468543171882629 epoch 7 batch 30 \n",
            "total loss: 0.5750157237052917 epoch 7 batch 35 \n",
            "total loss: 0.6100180745124817 epoch 7 batch 40 \n",
            "total loss: 0.5322399735450745 epoch 7 batch 45 \n",
            "total loss: 0.5852002501487732 epoch 7 batch 50 \n",
            "total loss: 0.5541520714759827 epoch 7 batch 55 \n",
            "total loss: 0.6354951858520508 epoch 7 batch 60 \n",
            "total loss: 0.5709330439567566 epoch 7 batch 65 \n",
            "total loss: 0.5509901642799377 epoch 7 batch 70 \n",
            "total loss: 0.6352809071540833 epoch 7 batch 75 \n",
            "total loss: 0.525368869304657 epoch 7 batch 80 \n",
            "total loss: 0.6758013367652893 epoch 7 batch 85 \n",
            "total loss: 0.538067638874054 epoch 7 batch 90 \n",
            "total loss: 0.6282547116279602 epoch 7 batch 95 \n",
            "total loss: 0.5717769861221313 epoch 7 batch 100 \n",
            "total loss: 0.5925206542015076 epoch 7 batch 105 \n",
            "total loss: 0.5748310089111328 epoch 7 batch 110 \n",
            "total loss: 0.5999457240104675 epoch 7 batch 115 \n",
            "total loss: 0.5801599621772766 epoch 7 batch 120 \n",
            "total loss: 0.6412753462791443 epoch 7 batch 125 \n",
            "total loss: 0.36302366852760315 epoch 8 batch 5 \n",
            "total loss: 0.4645155370235443 epoch 8 batch 10 \n",
            "total loss: 0.39205268025398254 epoch 8 batch 15 \n",
            "total loss: 0.40867742896080017 epoch 8 batch 20 \n",
            "total loss: 0.4239325225353241 epoch 8 batch 25 \n",
            "total loss: 0.4702189266681671 epoch 8 batch 30 \n",
            "total loss: 0.48485419154167175 epoch 8 batch 35 \n",
            "total loss: 0.4113442897796631 epoch 8 batch 40 \n",
            "total loss: 0.40504494309425354 epoch 8 batch 45 \n",
            "total loss: 0.5160152912139893 epoch 8 batch 50 \n",
            "total loss: 0.515261709690094 epoch 8 batch 55 \n",
            "total loss: 0.4537682831287384 epoch 8 batch 60 \n",
            "total loss: 0.5025420188903809 epoch 8 batch 65 \n",
            "total loss: 0.49189212918281555 epoch 8 batch 70 \n",
            "total loss: 0.4408739507198334 epoch 8 batch 75 \n",
            "total loss: 0.43847420811653137 epoch 8 batch 80 \n",
            "total loss: 0.5919972658157349 epoch 8 batch 85 \n",
            "total loss: 0.5030708909034729 epoch 8 batch 90 \n",
            "total loss: 0.5296990275382996 epoch 8 batch 95 \n",
            "total loss: 0.5007750988006592 epoch 8 batch 100 \n",
            "total loss: 0.5058427453041077 epoch 8 batch 105 \n",
            "total loss: 0.4588031470775604 epoch 8 batch 110 \n",
            "total loss: 0.5017201900482178 epoch 8 batch 115 \n",
            "total loss: 0.46871861815452576 epoch 8 batch 120 \n",
            "total loss: 0.4430551528930664 epoch 8 batch 125 \n",
            "total loss: 0.3216955363750458 epoch 9 batch 5 \n",
            "total loss: 0.364190936088562 epoch 9 batch 10 \n",
            "total loss: 0.3305354118347168 epoch 9 batch 15 \n",
            "total loss: 0.38101741671562195 epoch 9 batch 20 \n",
            "total loss: 0.3271498680114746 epoch 9 batch 25 \n",
            "total loss: 0.3657739460468292 epoch 9 batch 30 \n",
            "total loss: 0.41754743456840515 epoch 9 batch 35 \n",
            "total loss: 0.3447129726409912 epoch 9 batch 40 \n",
            "total loss: 0.3937135934829712 epoch 9 batch 45 \n",
            "total loss: 0.3587925136089325 epoch 9 batch 50 \n",
            "total loss: 0.3783649504184723 epoch 9 batch 55 \n",
            "total loss: 0.38081756234169006 epoch 9 batch 60 \n",
            "total loss: 0.38858410716056824 epoch 9 batch 65 \n",
            "total loss: 0.4099717140197754 epoch 9 batch 70 \n",
            "total loss: 0.34684550762176514 epoch 9 batch 75 \n",
            "total loss: 0.36341461539268494 epoch 9 batch 80 \n",
            "total loss: 0.39247679710388184 epoch 9 batch 85 \n",
            "total loss: 0.4482930600643158 epoch 9 batch 90 \n",
            "total loss: 0.4138124883174896 epoch 9 batch 95 \n",
            "total loss: 0.4838036298751831 epoch 9 batch 100 \n",
            "total loss: 0.4262460768222809 epoch 9 batch 105 \n",
            "total loss: 0.409709095954895 epoch 9 batch 110 \n",
            "total loss: 0.48572278022766113 epoch 9 batch 115 \n",
            "total loss: 0.4917674958705902 epoch 9 batch 120 \n",
            "total loss: 0.43850231170654297 epoch 9 batch 125 \n",
            "total loss: 0.2779780924320221 epoch 10 batch 5 \n",
            "total loss: 0.27656325697898865 epoch 10 batch 10 \n",
            "total loss: 0.2441144585609436 epoch 10 batch 15 \n",
            "total loss: 0.2926785945892334 epoch 10 batch 20 \n",
            "total loss: 0.2671527564525604 epoch 10 batch 25 \n",
            "total loss: 0.30136579275131226 epoch 10 batch 30 \n",
            "total loss: 0.28425610065460205 epoch 10 batch 35 \n",
            "total loss: 0.30533650517463684 epoch 10 batch 40 \n",
            "total loss: 0.27847370505332947 epoch 10 batch 45 \n",
            "total loss: 0.3157922029495239 epoch 10 batch 50 \n",
            "total loss: 0.30704912543296814 epoch 10 batch 55 \n",
            "total loss: 0.35295185446739197 epoch 10 batch 60 \n",
            "total loss: 0.30547574162483215 epoch 10 batch 65 \n",
            "total loss: 0.374978631734848 epoch 10 batch 70 \n",
            "total loss: 0.36432328820228577 epoch 10 batch 75 \n",
            "total loss: 0.4185580909252167 epoch 10 batch 80 \n",
            "total loss: 0.3557032644748688 epoch 10 batch 85 \n",
            "total loss: 0.33314234018325806 epoch 10 batch 90 \n",
            "total loss: 0.3223719298839569 epoch 10 batch 95 \n",
            "total loss: 0.36768674850463867 epoch 10 batch 100 \n",
            "total loss: 0.37393561005592346 epoch 10 batch 105 \n",
            "total loss: 0.3389917314052582 epoch 10 batch 110 \n",
            "total loss: 0.3147743046283722 epoch 10 batch 115 \n",
            "total loss: 0.3550471365451813 epoch 10 batch 120 \n",
            "total loss: 0.33843064308166504 epoch 10 batch 125 \n",
            "total loss: 0.2242337018251419 epoch 11 batch 5 \n",
            "total loss: 0.28140124678611755 epoch 11 batch 10 \n",
            "total loss: 0.2075161188840866 epoch 11 batch 15 \n",
            "total loss: 0.33373358845710754 epoch 11 batch 20 \n",
            "total loss: 0.2731974422931671 epoch 11 batch 25 \n",
            "total loss: 0.32650914788246155 epoch 11 batch 30 \n",
            "total loss: 0.2583295404911041 epoch 11 batch 35 \n",
            "total loss: 0.2773059904575348 epoch 11 batch 40 \n",
            "total loss: 0.23739837110042572 epoch 11 batch 45 \n",
            "total loss: 0.2838152348995209 epoch 11 batch 50 \n",
            "total loss: 0.26939359307289124 epoch 11 batch 55 \n",
            "total loss: 0.23923437297344208 epoch 11 batch 60 \n",
            "total loss: 0.27589163184165955 epoch 11 batch 65 \n",
            "total loss: 0.28848814964294434 epoch 11 batch 70 \n",
            "total loss: 0.26843515038490295 epoch 11 batch 75 \n",
            "total loss: 0.24608425796031952 epoch 11 batch 80 \n",
            "total loss: 0.25273647904396057 epoch 11 batch 85 \n",
            "total loss: 0.3228651285171509 epoch 11 batch 90 \n",
            "total loss: 0.2977347671985626 epoch 11 batch 95 \n",
            "total loss: 0.27997708320617676 epoch 11 batch 100 \n",
            "total loss: 0.33513903617858887 epoch 11 batch 105 \n",
            "total loss: 0.2964441478252411 epoch 11 batch 110 \n",
            "total loss: 0.3022097051143646 epoch 11 batch 115 \n",
            "total loss: 0.27854257822036743 epoch 11 batch 120 \n",
            "total loss: 0.30827394127845764 epoch 11 batch 125 \n",
            "total loss: 0.17861424386501312 epoch 12 batch 5 \n",
            "total loss: 0.21356640756130219 epoch 12 batch 10 \n",
            "total loss: 0.18638141453266144 epoch 12 batch 15 \n",
            "total loss: 0.16524577140808105 epoch 12 batch 20 \n",
            "total loss: 0.20892339944839478 epoch 12 batch 25 \n",
            "total loss: 0.21839487552642822 epoch 12 batch 30 \n",
            "total loss: 0.2295539826154709 epoch 12 batch 35 \n",
            "total loss: 0.27217480540275574 epoch 12 batch 40 \n",
            "total loss: 0.22751329839229584 epoch 12 batch 45 \n",
            "total loss: 0.23192371428012848 epoch 12 batch 50 \n",
            "total loss: 0.22609466314315796 epoch 12 batch 55 \n",
            "total loss: 0.28289300203323364 epoch 12 batch 60 \n",
            "total loss: 0.23666279017925262 epoch 12 batch 65 \n",
            "total loss: 0.2391248196363449 epoch 12 batch 70 \n",
            "total loss: 0.23386900126934052 epoch 12 batch 75 \n",
            "total loss: 0.266353577375412 epoch 12 batch 80 \n",
            "total loss: 0.2760936915874481 epoch 12 batch 85 \n",
            "total loss: 0.24293126165866852 epoch 12 batch 90 \n",
            "total loss: 0.273291677236557 epoch 12 batch 95 \n",
            "total loss: 0.29434308409690857 epoch 12 batch 100 \n",
            "total loss: 0.34582123160362244 epoch 12 batch 105 \n",
            "total loss: 0.24471580982208252 epoch 12 batch 110 \n",
            "total loss: 0.2554084062576294 epoch 12 batch 115 \n",
            "total loss: 0.2670764625072479 epoch 12 batch 120 \n",
            "total loss: 0.2912803292274475 epoch 12 batch 125 \n",
            "total loss: 0.15443141758441925 epoch 13 batch 5 \n",
            "total loss: 0.1931232064962387 epoch 13 batch 10 \n",
            "total loss: 0.17622584104537964 epoch 13 batch 15 \n",
            "total loss: 0.11989440768957138 epoch 13 batch 20 \n",
            "total loss: 0.17050595581531525 epoch 13 batch 25 \n",
            "total loss: 0.22286675870418549 epoch 13 batch 30 \n",
            "total loss: 0.17491263151168823 epoch 13 batch 35 \n",
            "total loss: 0.16707469522953033 epoch 13 batch 40 \n",
            "total loss: 0.22575640678405762 epoch 13 batch 45 \n",
            "total loss: 0.20654277503490448 epoch 13 batch 50 \n",
            "total loss: 0.18388397991657257 epoch 13 batch 55 \n",
            "total loss: 0.20496845245361328 epoch 13 batch 60 \n",
            "total loss: 0.22265975177288055 epoch 13 batch 65 \n",
            "total loss: 0.20060396194458008 epoch 13 batch 70 \n",
            "total loss: 0.22996003925800323 epoch 13 batch 75 \n",
            "total loss: 0.2857816517353058 epoch 13 batch 80 \n",
            "total loss: 0.2536607086658478 epoch 13 batch 85 \n",
            "total loss: 0.22935456037521362 epoch 13 batch 90 \n",
            "total loss: 0.2020351141691208 epoch 13 batch 95 \n",
            "total loss: 0.23328691720962524 epoch 13 batch 100 \n",
            "total loss: 0.19729816913604736 epoch 13 batch 105 \n",
            "total loss: 0.19689418375492096 epoch 13 batch 110 \n",
            "total loss: 0.27182623744010925 epoch 13 batch 115 \n",
            "total loss: 0.2491557002067566 epoch 13 batch 120 \n",
            "total loss: 0.23279501497745514 epoch 13 batch 125 \n",
            "total loss: 0.12885530292987823 epoch 14 batch 5 \n",
            "total loss: 0.17443670332431793 epoch 14 batch 10 \n",
            "total loss: 0.1631116420030594 epoch 14 batch 15 \n",
            "total loss: 0.2052302360534668 epoch 14 batch 20 \n",
            "total loss: 0.16438055038452148 epoch 14 batch 25 \n",
            "total loss: 0.13824032247066498 epoch 14 batch 30 \n",
            "total loss: 0.18673503398895264 epoch 14 batch 35 \n",
            "total loss: 0.16531902551651 epoch 14 batch 40 \n",
            "total loss: 0.1692447066307068 epoch 14 batch 45 \n",
            "total loss: 0.19206173717975616 epoch 14 batch 50 \n",
            "total loss: 0.1959967017173767 epoch 14 batch 55 \n",
            "total loss: 0.19990883767604828 epoch 14 batch 60 \n",
            "total loss: 0.19903121888637543 epoch 14 batch 65 \n",
            "total loss: 0.16730205714702606 epoch 14 batch 70 \n",
            "total loss: 0.18232785165309906 epoch 14 batch 75 \n",
            "total loss: 0.1787325143814087 epoch 14 batch 80 \n",
            "total loss: 0.21226747334003448 epoch 14 batch 85 \n",
            "total loss: 0.19593538343906403 epoch 14 batch 90 \n",
            "total loss: 0.1993504762649536 epoch 14 batch 95 \n",
            "total loss: 0.14179350435733795 epoch 14 batch 100 \n",
            "total loss: 0.17924420535564423 epoch 14 batch 105 \n",
            "total loss: 0.18711091578006744 epoch 14 batch 110 \n",
            "total loss: 0.18976633250713348 epoch 14 batch 115 \n",
            "total loss: 0.22911874949932098 epoch 14 batch 120 \n",
            "total loss: 0.19718317687511444 epoch 14 batch 125 \n",
            "total loss: 0.17037564516067505 epoch 15 batch 5 \n",
            "total loss: 0.13246919214725494 epoch 15 batch 10 \n",
            "total loss: 0.1645810604095459 epoch 15 batch 15 \n",
            "total loss: 0.18222437798976898 epoch 15 batch 20 \n",
            "total loss: 0.1660429984331131 epoch 15 batch 25 \n",
            "total loss: 0.15991497039794922 epoch 15 batch 30 \n",
            "total loss: 0.13443173468112946 epoch 15 batch 35 \n",
            "total loss: 0.18114899098873138 epoch 15 batch 40 \n",
            "total loss: 0.16897468268871307 epoch 15 batch 45 \n",
            "total loss: 0.1324223428964615 epoch 15 batch 50 \n",
            "total loss: 0.14253081381320953 epoch 15 batch 55 \n",
            "total loss: 0.14446453750133514 epoch 15 batch 60 \n",
            "total loss: 0.19215857982635498 epoch 15 batch 65 \n",
            "total loss: 0.13912205398082733 epoch 15 batch 70 \n",
            "total loss: 0.238775372505188 epoch 15 batch 75 \n",
            "total loss: 0.1749061942100525 epoch 15 batch 80 \n",
            "total loss: 0.1496431529521942 epoch 15 batch 85 \n",
            "total loss: 0.1542869657278061 epoch 15 batch 90 \n",
            "total loss: 0.16420096158981323 epoch 15 batch 95 \n",
            "total loss: 0.16588036715984344 epoch 15 batch 100 \n",
            "total loss: 0.21572370827198029 epoch 15 batch 105 \n",
            "total loss: 0.1784297227859497 epoch 15 batch 110 \n",
            "total loss: 0.2318013310432434 epoch 15 batch 115 \n",
            "total loss: 0.18491055071353912 epoch 15 batch 120 \n",
            "total loss: 0.20541547238826752 epoch 15 batch 125 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPYWakzu5qRu"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mR4Yiza5rq7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}