{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('tf2': conda)",
   "display_name": "Python 3.8.5 64-bit ('tf2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "d27b353191711a6c9af9d33e84b6e44ba80c162e4dfccdea388ab017ac6f80e8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.datasets import make_moons\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = make_moons(3000, noise=0.05)[0].astype(\"float32\")\n",
    "norm = layers.experimental.preprocessing.Normalization()\n",
    "norm.adapt(data)\n",
    "normalized_data = norm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine coupling layer\n",
    "# Creating a custom layer with keras API.\n",
    "output_dim = 256\n",
    "reg = 0.01\n",
    "\n",
    "\n",
    "def Coupling(input_shape):\n",
    "    input = keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    t_layer_1 = keras.layers.Dense(\n",
    "        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n",
    "    )(input)\n",
    "    t_layer_2 = keras.layers.Dense(\n",
    "        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n",
    "    )(t_layer_1)\n",
    "    t_layer_3 = keras.layers.Dense(\n",
    "        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n",
    "    )(t_layer_2)\n",
    "    t_layer_4 = keras.layers.Dense(\n",
    "        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n",
    "    )(t_layer_3)\n",
    "    t_layer_5 = keras.layers.Dense(\n",
    "        input_shape, activation=\"linear\", kernel_regularizer=regularizers.l2(reg)\n",
    "    )(t_layer_4)\n",
    "\n",
    "    s_layer_1 = keras.layers.Dense(\n",
    "        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n",
    "    )(input)\n",
    "    s_layer_2 = keras.layers.Dense(\n",
    "        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n",
    "    )(s_layer_1)\n",
    "    s_layer_3 = keras.layers.Dense(\n",
    "        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n",
    "    )(s_layer_2)\n",
    "    s_layer_4 = keras.layers.Dense(\n",
    "        output_dim, activation=\"relu\", kernel_regularizer=regularizers.l2(reg)\n",
    "    )(s_layer_3)\n",
    "    s_layer_5 = keras.layers.Dense(\n",
    "        input_shape, activation=\"tanh\", kernel_regularizer=regularizers.l2(reg)\n",
    "    )(s_layer_4)\n",
    "\n",
    "    return keras.Model(inputs=input, outputs=[s_layer_5, t_layer_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real NVP\n",
    "class RealNVP(keras.Model):\n",
    "    def __init__(self, num_coupling_layers):\n",
    "        super(RealNVP, self).__init__()\n",
    "\n",
    "        self.num_coupling_layers = num_coupling_layers\n",
    "\n",
    "        # Distribution of the latent space.\n",
    "        self.distribution = tfp.distributions.MultivariateNormalDiag(\n",
    "            loc=[0.0, 0.0], scale_diag=[1.0, 1.0]\n",
    "        )\n",
    "        self.masks = np.array(\n",
    "            [[0, 1], [1, 0]] * (num_coupling_layers // 2), dtype=\"float32\"\n",
    "        )\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.layers_list = [Coupling(2) for i in range(num_coupling_layers)]\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        \"\"\"List of the model's metrics.\n",
    "        We make sure the loss tracker is listed as part of `model.metrics`\n",
    "        so that `fit()` and `evaluate()` are able to `reset()` the loss tracker\n",
    "        at the start of each epoch and at the start of an `evaluate()` call.\n",
    "        \"\"\"\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, x, training=True):\n",
    "        log_det_inv = 0\n",
    "        direction = 1\n",
    "        if training:\n",
    "            direction = -1\n",
    "        for i in range(self.num_coupling_layers)[::direction]:\n",
    "            x_masked = x * self.masks[i]\n",
    "            reversed_mask = 1 - self.masks[i]\n",
    "            s, t = self.layers_list[i](x_masked)\n",
    "            s *= reversed_mask\n",
    "            t *= reversed_mask\n",
    "            gate = (direction - 1) / 2\n",
    "            x = (\n",
    "                reversed_mask\n",
    "                * (x * tf.exp(direction * s) + direction * t * tf.exp(gate * s))\n",
    "                + x_masked\n",
    "            )\n",
    "            log_det_inv += gate * tf.reduce_sum(s, [1])\n",
    "\n",
    "        return x, log_det_inv\n",
    "\n",
    "    # Log likelihood of the normal distribution plus the log determinant of the jacobian.\n",
    "\n",
    "    def log_loss(self, x):\n",
    "        y, logdet = self(x)\n",
    "        log_likelihood = self.distribution.log_prob(y) + logdet\n",
    "        return -tf.reduce_mean(log_likelihood)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            loss = self.log_loss(data)\n",
    "\n",
    "        g = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(g, self.trainable_variables))\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self.log_loss(data)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/300\n",
      "10/10 - 3s - loss: 2.7359 - val_loss: 2.6102\n",
      "Epoch 2/300\n",
      "10/10 - 0s - loss: 2.6281 - val_loss: 2.5687\n",
      "Epoch 3/300\n",
      "10/10 - 0s - loss: 2.5782 - val_loss: 2.5117\n",
      "Epoch 4/300\n",
      "10/10 - 0s - loss: 2.5319 - val_loss: 2.4697\n",
      "Epoch 5/300\n",
      "10/10 - 0s - loss: 2.4851 - val_loss: 2.4300\n",
      "Epoch 6/300\n",
      "10/10 - 0s - loss: 2.4456 - val_loss: 2.3987\n",
      "Epoch 7/300\n",
      "10/10 - 0s - loss: 2.4094 - val_loss: 2.3814\n",
      "Epoch 8/300\n",
      "10/10 - 0s - loss: 2.3748 - val_loss: 2.3564\n",
      "Epoch 9/300\n",
      "10/10 - 0s - loss: 2.3405 - val_loss: 2.3474\n",
      "Epoch 10/300\n",
      "10/10 - 0s - loss: 2.3193 - val_loss: 2.3207\n",
      "Epoch 11/300\n",
      "10/10 - 0s - loss: 2.2958 - val_loss: 2.2815\n",
      "Epoch 12/300\n",
      "10/10 - 0s - loss: 2.2618 - val_loss: 2.2676\n",
      "Epoch 13/300\n",
      "10/10 - 0s - loss: 2.2272 - val_loss: 2.2370\n",
      "Epoch 14/300\n",
      "10/10 - 0s - loss: 2.1962 - val_loss: 2.2371\n",
      "Epoch 15/300\n",
      "10/10 - 0s - loss: 2.1783 - val_loss: 2.2013\n",
      "Epoch 16/300\n",
      "10/10 - 0s - loss: 2.1535 - val_loss: 2.1750\n",
      "Epoch 17/300\n",
      "10/10 - 0s - loss: 2.1242 - val_loss: 2.1427\n",
      "Epoch 18/300\n",
      "10/10 - 0s - loss: 2.1027 - val_loss: 2.1054\n",
      "Epoch 19/300\n",
      "10/10 - 0s - loss: 2.1024 - val_loss: 2.0925\n",
      "Epoch 20/300\n",
      "10/10 - 0s - loss: 2.0667 - val_loss: 2.0988\n",
      "Epoch 21/300\n",
      "10/10 - 0s - loss: 2.0546 - val_loss: 2.0973\n",
      "Epoch 22/300\n",
      "10/10 - 0s - loss: 2.0294 - val_loss: 2.1028\n",
      "Epoch 23/300\n",
      "10/10 - 0s - loss: 2.1151 - val_loss: 2.1300\n",
      "Epoch 24/300\n",
      "10/10 - 0s - loss: 2.0284 - val_loss: 2.0281\n",
      "Epoch 25/300\n",
      "10/10 - 0s - loss: 1.9479 - val_loss: 1.9835\n",
      "Epoch 26/300\n",
      "10/10 - 0s - loss: 1.8918 - val_loss: 1.9174\n",
      "Epoch 27/300\n",
      "10/10 - 0s - loss: 1.8359 - val_loss: 1.8858\n",
      "Epoch 28/300\n",
      "10/10 - 0s - loss: 1.7650 - val_loss: 1.8817\n",
      "Epoch 29/300\n",
      "10/10 - 0s - loss: 1.7478 - val_loss: 1.8022\n",
      "Epoch 30/300\n",
      "10/10 - 0s - loss: 1.6890 - val_loss: 1.7708\n",
      "Epoch 31/300\n",
      "10/10 - 0s - loss: 1.6900 - val_loss: 1.7917\n",
      "Epoch 32/300\n",
      "10/10 - 0s - loss: 1.6946 - val_loss: 1.8139\n",
      "Epoch 33/300\n",
      "10/10 - 0s - loss: 1.7285 - val_loss: 1.9783\n",
      "Epoch 34/300\n",
      "10/10 - 0s - loss: 1.8187 - val_loss: 1.8347\n",
      "Epoch 35/300\n",
      "10/10 - 0s - loss: 1.6793 - val_loss: 1.7004\n",
      "Epoch 36/300\n",
      "10/10 - 0s - loss: 1.6545 - val_loss: 1.6856\n",
      "Epoch 37/300\n",
      "10/10 - 0s - loss: 1.6492 - val_loss: 1.7314\n",
      "Epoch 38/300\n",
      "10/10 - 0s - loss: 1.6009 - val_loss: 1.6789\n",
      "Epoch 39/300\n",
      "10/10 - 0s - loss: 1.5684 - val_loss: 1.7240\n",
      "Epoch 40/300\n",
      "10/10 - 0s - loss: 1.5920 - val_loss: 1.6681\n",
      "Epoch 41/300\n",
      "10/10 - 0s - loss: 1.5254 - val_loss: 1.5924\n",
      "Epoch 42/300\n",
      "10/10 - 0s - loss: 1.4909 - val_loss: 1.6582\n",
      "Epoch 43/300\n",
      "10/10 - 0s - loss: 1.5365 - val_loss: 1.6357\n",
      "Epoch 44/300\n",
      "10/10 - 0s - loss: 1.4725 - val_loss: 1.5530\n",
      "Epoch 45/300\n",
      "10/10 - 0s - loss: 1.4458 - val_loss: 1.5587\n",
      "Epoch 46/300\n",
      "10/10 - 0s - loss: 1.4150 - val_loss: 1.6433\n",
      "Epoch 47/300\n",
      "10/10 - 0s - loss: 1.4333 - val_loss: 1.5100\n",
      "Epoch 48/300\n",
      "10/10 - 0s - loss: 1.4119 - val_loss: 1.4878\n",
      "Epoch 49/300\n",
      "10/10 - 0s - loss: 1.3924 - val_loss: 1.4557\n",
      "Epoch 50/300\n",
      "10/10 - 0s - loss: 1.3846 - val_loss: 1.4960\n",
      "Epoch 51/300\n",
      "10/10 - 0s - loss: 1.3951 - val_loss: 1.4971\n",
      "Epoch 52/300\n",
      "10/10 - 0s - loss: 1.4003 - val_loss: 1.4651\n",
      "Epoch 53/300\n",
      "10/10 - 0s - loss: 1.3571 - val_loss: 1.5804\n",
      "Epoch 54/300\n",
      "10/10 - 0s - loss: 1.3786 - val_loss: 1.4588\n",
      "Epoch 55/300\n",
      "10/10 - 0s - loss: 1.3510 - val_loss: 1.4077\n",
      "Epoch 56/300\n",
      "10/10 - 0s - loss: 1.3498 - val_loss: 1.4069\n",
      "Epoch 57/300\n",
      "10/10 - 0s - loss: 1.3724 - val_loss: 1.4201\n",
      "Epoch 58/300\n",
      "10/10 - 0s - loss: 1.3515 - val_loss: 1.3984\n",
      "Epoch 59/300\n",
      "10/10 - 0s - loss: 1.3289 - val_loss: 1.4039\n",
      "Epoch 60/300\n",
      "10/10 - 0s - loss: 1.3228 - val_loss: 1.3708\n",
      "Epoch 61/300\n",
      "10/10 - 0s - loss: 1.2959 - val_loss: 1.4140\n",
      "Epoch 62/300\n",
      "10/10 - 0s - loss: 1.2799 - val_loss: 1.3744\n",
      "Epoch 63/300\n",
      "10/10 - 0s - loss: 1.2911 - val_loss: 1.4967\n",
      "Epoch 64/300\n",
      "10/10 - 0s - loss: 1.2955 - val_loss: 1.3612\n",
      "Epoch 65/300\n",
      "10/10 - 0s - loss: 1.2807 - val_loss: 1.3548\n",
      "Epoch 66/300\n",
      "10/10 - 0s - loss: 1.2908 - val_loss: 1.3156\n",
      "Epoch 67/300\n",
      "10/10 - 0s - loss: 1.2701 - val_loss: 1.3035\n",
      "Epoch 68/300\n",
      "10/10 - 0s - loss: 1.2593 - val_loss: 1.2859\n",
      "Epoch 69/300\n",
      "10/10 - 0s - loss: 1.2586 - val_loss: 1.3026\n",
      "Epoch 70/300\n",
      "10/10 - 0s - loss: 1.2642 - val_loss: 1.3533\n",
      "Epoch 71/300\n",
      "10/10 - 0s - loss: 1.2869 - val_loss: 1.3132\n",
      "Epoch 72/300\n",
      "10/10 - 0s - loss: 1.2667 - val_loss: 1.3153\n",
      "Epoch 73/300\n",
      "10/10 - 0s - loss: 1.2466 - val_loss: 1.3068\n",
      "Epoch 74/300\n",
      "10/10 - 0s - loss: 1.2449 - val_loss: 1.3623\n",
      "Epoch 75/300\n",
      "10/10 - 0s - loss: 1.2421 - val_loss: 1.3046\n",
      "Epoch 76/300\n",
      "10/10 - 0s - loss: 1.2300 - val_loss: 1.4259\n",
      "Epoch 77/300\n",
      "10/10 - 0s - loss: 1.2339 - val_loss: 1.2971\n",
      "Epoch 78/300\n",
      "10/10 - 0s - loss: 1.2408 - val_loss: 1.3384\n",
      "Epoch 79/300\n",
      "10/10 - 0s - loss: 1.2374 - val_loss: 1.3134\n",
      "Epoch 80/300\n",
      "10/10 - 0s - loss: 1.2310 - val_loss: 1.3983\n",
      "Epoch 81/300\n",
      "10/10 - 0s - loss: 1.2407 - val_loss: 1.3294\n",
      "Epoch 82/300\n",
      "10/10 - 0s - loss: 1.2602 - val_loss: 1.3791\n",
      "Epoch 83/300\n",
      "10/10 - 0s - loss: 1.2823 - val_loss: 1.6116\n",
      "Epoch 84/300\n",
      "10/10 - 0s - loss: 1.2861 - val_loss: 1.3066\n",
      "Epoch 85/300\n",
      "10/10 - 0s - loss: 1.2561 - val_loss: 1.3915\n",
      "Epoch 86/300\n",
      "10/10 - 0s - loss: 1.2488 - val_loss: 1.3306\n",
      "Epoch 87/300\n",
      "10/10 - 0s - loss: 1.2414 - val_loss: 1.3277\n",
      "Epoch 88/300\n",
      "10/10 - 0s - loss: 1.2413 - val_loss: 1.3406\n",
      "Epoch 89/300\n",
      "10/10 - 0s - loss: 1.2328 - val_loss: 1.3403\n",
      "Epoch 90/300\n",
      "10/10 - 0s - loss: 1.2347 - val_loss: 1.3324\n",
      "Epoch 91/300\n",
      "10/10 - 0s - loss: 1.2241 - val_loss: 1.3388\n",
      "Epoch 92/300\n",
      "10/10 - 0s - loss: 1.2381 - val_loss: 1.3257\n",
      "Epoch 93/300\n",
      "10/10 - 0s - loss: 1.2239 - val_loss: 1.3376\n",
      "Epoch 94/300\n",
      "10/10 - 0s - loss: 1.2412 - val_loss: 1.3032\n",
      "Epoch 95/300\n",
      "10/10 - 0s - loss: 1.2143 - val_loss: 1.3173\n",
      "Epoch 96/300\n",
      "10/10 - 0s - loss: 1.2167 - val_loss: 1.2876\n",
      "Epoch 97/300\n",
      "10/10 - 0s - loss: 1.2109 - val_loss: 1.3712\n",
      "Epoch 98/300\n",
      "10/10 - 0s - loss: 1.2265 - val_loss: 1.4391\n",
      "Epoch 99/300\n",
      "10/10 - 0s - loss: 1.2364 - val_loss: 1.3378\n",
      "Epoch 100/300\n",
      "10/10 - 0s - loss: 1.2210 - val_loss: 1.3395\n",
      "Epoch 101/300\n",
      "10/10 - 0s - loss: 1.2482 - val_loss: 1.4088\n",
      "Epoch 102/300\n",
      "10/10 - 0s - loss: 1.2367 - val_loss: 1.3097\n",
      "Epoch 103/300\n",
      "10/10 - 0s - loss: 1.2197 - val_loss: 1.3194\n",
      "Epoch 104/300\n",
      "10/10 - 0s - loss: 1.1969 - val_loss: 1.3658\n",
      "Epoch 105/300\n",
      "10/10 - 0s - loss: 1.1937 - val_loss: 1.3608\n",
      "Epoch 106/300\n",
      "10/10 - 0s - loss: 1.2252 - val_loss: 1.3212\n",
      "Epoch 107/300\n",
      "10/10 - 0s - loss: 1.2323 - val_loss: 1.3671\n",
      "Epoch 108/300\n",
      "10/10 - 0s - loss: 1.2409 - val_loss: 1.3479\n",
      "Epoch 109/300\n",
      "10/10 - 0s - loss: 1.2173 - val_loss: 1.3277\n",
      "Epoch 110/300\n",
      "10/10 - 0s - loss: 1.2319 - val_loss: 1.3114\n",
      "Epoch 111/300\n",
      "10/10 - 0s - loss: 1.2052 - val_loss: 1.4191\n",
      "Epoch 112/300\n",
      "10/10 - 0s - loss: 1.2176 - val_loss: 1.3097\n",
      "Epoch 113/300\n",
      "10/10 - 0s - loss: 1.2030 - val_loss: 1.3082\n",
      "Epoch 114/300\n",
      "10/10 - 0s - loss: 1.2010 - val_loss: 1.3225\n",
      "Epoch 115/300\n",
      "10/10 - 0s - loss: 1.2218 - val_loss: 1.3096\n",
      "Epoch 116/300\n",
      "10/10 - 0s - loss: 1.2407 - val_loss: 1.4022\n",
      "Epoch 117/300\n",
      "10/10 - 0s - loss: 1.2496 - val_loss: 1.3535\n",
      "Epoch 118/300\n",
      "10/10 - 0s - loss: 1.2188 - val_loss: 1.3337\n",
      "Epoch 119/300\n",
      "10/10 - 0s - loss: 1.2193 - val_loss: 1.4269\n",
      "Epoch 120/300\n",
      "10/10 - 0s - loss: 1.2197 - val_loss: 1.2918\n",
      "Epoch 121/300\n",
      "10/10 - 0s - loss: 1.2129 - val_loss: 1.3135\n",
      "Epoch 122/300\n",
      "10/10 - 0s - loss: 1.2163 - val_loss: 1.3304\n",
      "Epoch 123/300\n",
      "10/10 - 0s - loss: 1.3866 - val_loss: 1.3923\n",
      "Epoch 124/300\n",
      "10/10 - 0s - loss: 1.3617 - val_loss: 1.3562\n",
      "Epoch 125/300\n",
      "10/10 - 0s - loss: 1.3042 - val_loss: 1.3225\n",
      "Epoch 126/300\n",
      "10/10 - 0s - loss: 1.2515 - val_loss: 1.3166\n",
      "Epoch 127/300\n",
      "10/10 - 0s - loss: 1.2405 - val_loss: 1.2803\n",
      "Epoch 128/300\n",
      "10/10 - 0s - loss: 1.2195 - val_loss: 1.2834\n",
      "Epoch 129/300\n",
      "10/10 - 0s - loss: 1.2168 - val_loss: 1.2783\n",
      "Epoch 130/300\n",
      "10/10 - 0s - loss: 1.2024 - val_loss: 1.2771\n",
      "Epoch 131/300\n",
      "10/10 - 0s - loss: 1.2111 - val_loss: 1.2504\n",
      "Epoch 132/300\n",
      "10/10 - 0s - loss: 1.2212 - val_loss: 1.3474\n",
      "Epoch 133/300\n",
      "10/10 - 0s - loss: 1.2182 - val_loss: 1.2937\n",
      "Epoch 134/300\n",
      "10/10 - 0s - loss: 1.2065 - val_loss: 1.3008\n",
      "Epoch 135/300\n",
      "10/10 - 0s - loss: 1.2093 - val_loss: 1.3361\n",
      "Epoch 136/300\n",
      "10/10 - 0s - loss: 1.2124 - val_loss: 1.3088\n",
      "Epoch 137/300\n",
      "10/10 - 0s - loss: 1.2308 - val_loss: 1.3742\n",
      "Epoch 138/300\n",
      "10/10 - 0s - loss: 1.2263 - val_loss: 1.2944\n",
      "Epoch 139/300\n",
      "10/10 - 0s - loss: 1.2263 - val_loss: 1.2721\n",
      "Epoch 140/300\n",
      "10/10 - 0s - loss: 1.2285 - val_loss: 1.2724\n",
      "Epoch 141/300\n",
      "10/10 - 0s - loss: 1.2242 - val_loss: 1.2648\n",
      "Epoch 142/300\n",
      "10/10 - 0s - loss: 1.2014 - val_loss: 1.3563\n",
      "Epoch 143/300\n",
      "10/10 - 0s - loss: 1.1929 - val_loss: 1.3402\n",
      "Epoch 144/300\n",
      "10/10 - 0s - loss: 1.1927 - val_loss: 1.3215\n",
      "Epoch 145/300\n",
      "10/10 - 0s - loss: 1.2082 - val_loss: 1.3479\n",
      "Epoch 146/300\n",
      "10/10 - 0s - loss: 1.2145 - val_loss: 1.2965\n",
      "Epoch 147/300\n",
      "10/10 - 0s - loss: 1.2093 - val_loss: 1.2986\n",
      "Epoch 148/300\n",
      "10/10 - 0s - loss: 1.1949 - val_loss: 1.2898\n",
      "Epoch 149/300\n",
      "10/10 - 0s - loss: 1.2043 - val_loss: 1.3118\n",
      "Epoch 150/300\n",
      "10/10 - 0s - loss: 1.1865 - val_loss: 1.3181\n",
      "Epoch 151/300\n",
      "10/10 - 0s - loss: 1.1916 - val_loss: 1.3167\n",
      "Epoch 152/300\n",
      "10/10 - 0s - loss: 1.1747 - val_loss: 1.3411\n",
      "Epoch 153/300\n",
      "10/10 - 0s - loss: 1.1992 - val_loss: 1.2860\n",
      "Epoch 154/300\n",
      "10/10 - 0s - loss: 1.2083 - val_loss: 1.2937\n",
      "Epoch 155/300\n",
      "10/10 - 0s - loss: 1.2013 - val_loss: 1.3606\n",
      "Epoch 156/300\n",
      "10/10 - 0s - loss: 1.1944 - val_loss: 1.3296\n",
      "Epoch 157/300\n",
      "10/10 - 0s - loss: 1.1971 - val_loss: 1.2816\n",
      "Epoch 158/300\n",
      "10/10 - 0s - loss: 1.2111 - val_loss: 1.3178\n",
      "Epoch 159/300\n",
      "10/10 - 0s - loss: 1.2027 - val_loss: 1.3254\n",
      "Epoch 160/300\n",
      "10/10 - 0s - loss: 1.2077 - val_loss: 1.3311\n",
      "Epoch 161/300\n",
      "10/10 - 0s - loss: 1.1954 - val_loss: 1.2750\n",
      "Epoch 162/300\n",
      "10/10 - 0s - loss: 1.1830 - val_loss: 1.2996\n",
      "Epoch 163/300\n",
      "10/10 - 0s - loss: 1.1737 - val_loss: 1.2925\n",
      "Epoch 164/300\n",
      "10/10 - 0s - loss: 1.1994 - val_loss: 1.3495\n",
      "Epoch 165/300\n",
      "10/10 - 0s - loss: 1.1931 - val_loss: 1.3335\n",
      "Epoch 166/300\n",
      "10/10 - 0s - loss: 1.1797 - val_loss: 1.3374\n",
      "Epoch 167/300\n",
      "10/10 - 0s - loss: 1.1985 - val_loss: 1.2800\n",
      "Epoch 168/300\n",
      "10/10 - 0s - loss: 1.1975 - val_loss: 1.3299\n",
      "Epoch 169/300\n",
      "10/10 - 0s - loss: 1.1886 - val_loss: 1.4519\n",
      "Epoch 170/300\n",
      "10/10 - 0s - loss: 1.2250 - val_loss: 1.2588\n",
      "Epoch 171/300\n",
      "10/10 - 0s - loss: 1.1894 - val_loss: 1.3449\n",
      "Epoch 172/300\n",
      "10/10 - 0s - loss: 1.1818 - val_loss: 1.2725\n",
      "Epoch 173/300\n",
      "10/10 - 0s - loss: 1.1946 - val_loss: 1.3558\n",
      "Epoch 174/300\n",
      "10/10 - 0s - loss: 1.2309 - val_loss: 1.2915\n",
      "Epoch 175/300\n",
      "10/10 - 0s - loss: 1.1895 - val_loss: 1.3167\n",
      "Epoch 176/300\n",
      "10/10 - 0s - loss: 1.1964 - val_loss: 1.2959\n",
      "Epoch 177/300\n",
      "10/10 - 0s - loss: 1.1861 - val_loss: 1.2910\n",
      "Epoch 178/300\n",
      "10/10 - 0s - loss: 1.1930 - val_loss: 1.3613\n",
      "Epoch 179/300\n",
      "10/10 - 0s - loss: 1.2034 - val_loss: 1.2737\n",
      "Epoch 180/300\n",
      "10/10 - 0s - loss: 1.2178 - val_loss: 1.3583\n",
      "Epoch 181/300\n",
      "10/10 - 0s - loss: 1.2179 - val_loss: 1.3274\n",
      "Epoch 182/300\n",
      "10/10 - 0s - loss: 1.1766 - val_loss: 1.3367\n",
      "Epoch 183/300\n",
      "10/10 - 0s - loss: 1.1947 - val_loss: 1.3175\n",
      "Epoch 184/300\n",
      "10/10 - 0s - loss: 1.1861 - val_loss: 1.3565\n",
      "Epoch 185/300\n",
      "10/10 - 0s - loss: 1.1843 - val_loss: 1.3098\n",
      "Epoch 186/300\n",
      "10/10 - 0s - loss: 1.1759 - val_loss: 1.3429\n",
      "Epoch 187/300\n",
      "10/10 - 0s - loss: 1.2061 - val_loss: 1.3189\n",
      "Epoch 188/300\n",
      "10/10 - 0s - loss: 1.1849 - val_loss: 1.3097\n",
      "Epoch 189/300\n",
      "10/10 - 0s - loss: 1.1716 - val_loss: 1.3532\n",
      "Epoch 190/300\n",
      "10/10 - 0s - loss: 1.1802 - val_loss: 1.2769\n",
      "Epoch 191/300\n",
      "10/10 - 0s - loss: 1.1775 - val_loss: 1.3497\n",
      "Epoch 192/300\n",
      "10/10 - 0s - loss: 1.1894 - val_loss: 1.3436\n",
      "Epoch 193/300\n",
      "10/10 - 0s - loss: 1.1805 - val_loss: 1.3297\n",
      "Epoch 194/300\n",
      "10/10 - 0s - loss: 1.2697 - val_loss: 1.3120\n",
      "Epoch 195/300\n",
      "10/10 - 0s - loss: 1.2443 - val_loss: 1.2796\n",
      "Epoch 196/300\n",
      "10/10 - 0s - loss: 1.2152 - val_loss: 1.3004\n",
      "Epoch 197/300\n",
      "10/10 - 0s - loss: 1.2040 - val_loss: 1.3563\n",
      "Epoch 198/300\n",
      "10/10 - 0s - loss: 1.1963 - val_loss: 1.3280\n",
      "Epoch 199/300\n",
      "10/10 - 0s - loss: 1.1990 - val_loss: 1.3620\n",
      "Epoch 200/300\n",
      "10/10 - 0s - loss: 1.1923 - val_loss: 1.2720\n",
      "Epoch 201/300\n",
      "10/10 - 0s - loss: 1.1953 - val_loss: 1.3313\n",
      "Epoch 202/300\n",
      "10/10 - 0s - loss: 1.1982 - val_loss: 1.2913\n",
      "Epoch 203/300\n",
      "10/10 - 0s - loss: 1.1894 - val_loss: 1.3804\n",
      "Epoch 204/300\n",
      "10/10 - 0s - loss: 1.1906 - val_loss: 1.3775\n",
      "Epoch 205/300\n",
      "10/10 - 0s - loss: 1.1834 - val_loss: 1.3291\n",
      "Epoch 206/300\n",
      "10/10 - 0s - loss: 1.1791 - val_loss: 1.2757\n",
      "Epoch 207/300\n",
      "10/10 - 0s - loss: 1.1839 - val_loss: 1.3727\n",
      "Epoch 208/300\n",
      "10/10 - 0s - loss: 1.1968 - val_loss: 1.3934\n",
      "Epoch 209/300\n",
      "10/10 - 0s - loss: 1.2070 - val_loss: 1.2868\n",
      "Epoch 210/300\n",
      "10/10 - 0s - loss: 1.1991 - val_loss: 1.2971\n",
      "Epoch 211/300\n",
      "10/10 - 0s - loss: 1.2005 - val_loss: 1.2744\n",
      "Epoch 212/300\n",
      "10/10 - 0s - loss: 1.1926 - val_loss: 1.3655\n",
      "Epoch 213/300\n",
      "10/10 - 0s - loss: 1.2001 - val_loss: 1.2713\n",
      "Epoch 214/300\n",
      "10/10 - 0s - loss: 1.1971 - val_loss: 1.3409\n",
      "Epoch 215/300\n",
      "10/10 - 0s - loss: 1.1913 - val_loss: 1.2547\n",
      "Epoch 216/300\n",
      "10/10 - 0s - loss: 1.1900 - val_loss: 1.2974\n",
      "Epoch 217/300\n",
      "10/10 - 0s - loss: 1.1845 - val_loss: 1.3271\n",
      "Epoch 218/300\n",
      "10/10 - 0s - loss: 1.1715 - val_loss: 1.3507\n",
      "Epoch 219/300\n",
      "10/10 - 0s - loss: 1.2147 - val_loss: 1.3115\n",
      "Epoch 220/300\n",
      "10/10 - 0s - loss: 1.2069 - val_loss: 1.2895\n",
      "Epoch 221/300\n",
      "10/10 - 0s - loss: 1.1743 - val_loss: 1.2903\n",
      "Epoch 222/300\n",
      "10/10 - 0s - loss: 1.2029 - val_loss: 1.3599\n",
      "Epoch 223/300\n",
      "10/10 - 0s - loss: 1.1863 - val_loss: 1.3038\n",
      "Epoch 224/300\n",
      "10/10 - 0s - loss: 1.2219 - val_loss: 1.3375\n",
      "Epoch 225/300\n",
      "10/10 - 0s - loss: 1.2108 - val_loss: 1.3088\n",
      "Epoch 226/300\n",
      "10/10 - 0s - loss: 1.1935 - val_loss: 1.3130\n",
      "Epoch 227/300\n",
      "10/10 - 0s - loss: 1.1791 - val_loss: 1.3309\n",
      "Epoch 228/300\n",
      "10/10 - 0s - loss: 1.1870 - val_loss: 1.3742\n",
      "Epoch 229/300\n",
      "10/10 - 0s - loss: 1.2082 - val_loss: 1.3150\n",
      "Epoch 230/300\n",
      "10/10 - 0s - loss: 1.2098 - val_loss: 1.3320\n",
      "Epoch 231/300\n",
      "10/10 - 0s - loss: 1.1772 - val_loss: 1.3606\n",
      "Epoch 232/300\n",
      "10/10 - 0s - loss: 1.1825 - val_loss: 1.3043\n",
      "Epoch 233/300\n",
      "10/10 - 0s - loss: 1.1862 - val_loss: 1.3846\n",
      "Epoch 234/300\n",
      "10/10 - 0s - loss: 1.1774 - val_loss: 1.2775\n",
      "Epoch 235/300\n",
      "10/10 - 0s - loss: 1.1736 - val_loss: 1.2989\n",
      "Epoch 236/300\n",
      "10/10 - 0s - loss: 1.1659 - val_loss: 1.3617\n",
      "Epoch 237/300\n",
      "10/10 - 0s - loss: 1.1747 - val_loss: 1.3181\n",
      "Epoch 238/300\n",
      "10/10 - 0s - loss: 1.1991 - val_loss: 1.2976\n",
      "Epoch 239/300\n",
      "10/10 - 0s - loss: 1.1921 - val_loss: 1.2995\n",
      "Epoch 240/300\n",
      "10/10 - 0s - loss: 1.1984 - val_loss: 1.3181\n",
      "Epoch 241/300\n",
      "10/10 - 0s - loss: 1.1742 - val_loss: 1.2805\n",
      "Epoch 242/300\n",
      "10/10 - 0s - loss: 1.1777 - val_loss: 1.3855\n",
      "Epoch 243/300\n",
      "10/10 - 0s - loss: 1.1795 - val_loss: 1.2771\n",
      "Epoch 244/300\n",
      "10/10 - 0s - loss: 1.1617 - val_loss: 1.2806\n",
      "Epoch 245/300\n",
      "10/10 - 0s - loss: 1.1651 - val_loss: 1.3296\n",
      "Epoch 246/300\n",
      "10/10 - 0s - loss: 1.1713 - val_loss: 1.3184\n",
      "Epoch 247/300\n",
      "10/10 - 0s - loss: 1.1824 - val_loss: 1.4214\n",
      "Epoch 248/300\n",
      "10/10 - 0s - loss: 1.1933 - val_loss: 1.3467\n",
      "Epoch 249/300\n",
      "10/10 - 0s - loss: 1.1835 - val_loss: 1.3708\n",
      "Epoch 250/300\n",
      "10/10 - 0s - loss: 1.1841 - val_loss: 1.4029\n",
      "Epoch 251/300\n",
      "10/10 - 0s - loss: 1.1803 - val_loss: 1.2658\n",
      "Epoch 252/300\n",
      "10/10 - 0s - loss: 1.1804 - val_loss: 1.3694\n",
      "Epoch 253/300\n",
      "10/10 - 0s - loss: 1.1721 - val_loss: 1.3303\n",
      "Epoch 254/300\n",
      "10/10 - 0s - loss: 1.1848 - val_loss: 1.3518\n",
      "Epoch 255/300\n",
      "10/10 - 0s - loss: 1.1842 - val_loss: 1.3233\n",
      "Epoch 256/300\n",
      "10/10 - 0s - loss: 1.1708 - val_loss: 1.3837\n",
      "Epoch 257/300\n",
      "10/10 - 0s - loss: 1.1845 - val_loss: 1.3656\n",
      "Epoch 258/300\n",
      "10/10 - 0s - loss: 1.1821 - val_loss: 1.3189\n",
      "Epoch 259/300\n",
      "10/10 - 0s - loss: 1.1771 - val_loss: 1.3241\n",
      "Epoch 260/300\n",
      "10/10 - 0s - loss: 1.1773 - val_loss: 1.4161\n",
      "Epoch 261/300\n",
      "10/10 - 0s - loss: 1.1948 - val_loss: 1.2495\n",
      "Epoch 262/300\n",
      "10/10 - 0s - loss: 1.1783 - val_loss: 1.3373\n",
      "Epoch 263/300\n",
      "10/10 - 0s - loss: 1.1878 - val_loss: 1.2794\n",
      "Epoch 264/300\n",
      "10/10 - 0s - loss: 1.1718 - val_loss: 1.3649\n",
      "Epoch 265/300\n",
      "10/10 - 0s - loss: 1.1772 - val_loss: 1.4080\n",
      "Epoch 266/300\n",
      "10/10 - 0s - loss: 1.1829 - val_loss: 1.2757\n",
      "Epoch 267/300\n",
      "10/10 - 0s - loss: 1.1705 - val_loss: 1.3635\n",
      "Epoch 268/300\n",
      "10/10 - 0s - loss: 1.1646 - val_loss: 1.3835\n",
      "Epoch 269/300\n",
      "10/10 - 0s - loss: 1.1877 - val_loss: 1.3168\n",
      "Epoch 270/300\n",
      "10/10 - 0s - loss: 1.1673 - val_loss: 1.3506\n",
      "Epoch 271/300\n",
      "10/10 - 0s - loss: 1.1887 - val_loss: 1.3115\n",
      "Epoch 272/300\n",
      "10/10 - 0s - loss: 1.1873 - val_loss: 1.3548\n",
      "Epoch 273/300\n",
      "10/10 - 0s - loss: 1.1797 - val_loss: 1.3516\n",
      "Epoch 274/300\n",
      "10/10 - 0s - loss: 1.1741 - val_loss: 1.3690\n",
      "Epoch 275/300\n",
      "10/10 - 0s - loss: 1.1792 - val_loss: 1.3210\n",
      "Epoch 276/300\n",
      "10/10 - 0s - loss: 1.1819 - val_loss: 1.4254\n",
      "Epoch 277/300\n",
      "10/10 - 0s - loss: 1.1846 - val_loss: 1.3133\n",
      "Epoch 278/300\n",
      "10/10 - 0s - loss: 1.1812 - val_loss: 1.4127\n",
      "Epoch 279/300\n",
      "10/10 - 0s - loss: 1.2593 - val_loss: 1.3614\n",
      "Epoch 280/300\n",
      "10/10 - 0s - loss: 1.2134 - val_loss: 1.3876\n",
      "Epoch 281/300\n",
      "10/10 - 0s - loss: 1.2082 - val_loss: 1.4172\n",
      "Epoch 282/300\n",
      "10/10 - 0s - loss: 1.1871 - val_loss: 1.3159\n",
      "Epoch 283/300\n",
      "10/10 - 0s - loss: 1.1942 - val_loss: 1.4023\n",
      "Epoch 284/300\n",
      "10/10 - 0s - loss: 1.1969 - val_loss: 1.2691\n",
      "Epoch 285/300\n",
      "10/10 - 0s - loss: 1.2006 - val_loss: 1.3483\n",
      "Epoch 286/300\n",
      "10/10 - 0s - loss: 1.2071 - val_loss: 1.3374\n",
      "Epoch 287/300\n",
      "10/10 - 0s - loss: 1.1941 - val_loss: 1.3296\n",
      "Epoch 288/300\n",
      "10/10 - 0s - loss: 1.1787 - val_loss: 1.3635\n",
      "Epoch 289/300\n",
      "10/10 - 0s - loss: 1.1820 - val_loss: 1.2899\n",
      "Epoch 290/300\n",
      "10/10 - 0s - loss: 1.2061 - val_loss: 1.2888\n",
      "Epoch 291/300\n",
      "10/10 - 0s - loss: 1.1927 - val_loss: 1.3531\n",
      "Epoch 292/300\n",
      "10/10 - 0s - loss: 1.2038 - val_loss: 1.2930\n",
      "Epoch 293/300\n",
      "10/10 - 0s - loss: 1.1932 - val_loss: 1.3876\n",
      "Epoch 294/300\n",
      "10/10 - 0s - loss: 1.1821 - val_loss: 1.2858\n",
      "Epoch 295/300\n",
      "10/10 - 0s - loss: 1.1646 - val_loss: 1.3497\n",
      "Epoch 296/300\n",
      "10/10 - 0s - loss: 1.1687 - val_loss: 1.2941\n",
      "Epoch 297/300\n",
      "10/10 - 0s - loss: 1.1645 - val_loss: 1.3277\n",
      "Epoch 298/300\n",
      "10/10 - 0s - loss: 1.1797 - val_loss: 1.3722\n",
      "Epoch 299/300\n",
      "10/10 - 0s - loss: 1.1718 - val_loss: 1.2972\n",
      "Epoch 300/300\n",
      "10/10 - 0s - loss: 1.1651 - val_loss: 1.3118\n"
     ]
    }
   ],
   "source": [
    "model = RealNVP(num_coupling_layers=6)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001))\n",
    "\n",
    "history = model.fit(\n",
    "    normalized_data, batch_size=256, epochs=300, verbose=2, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}