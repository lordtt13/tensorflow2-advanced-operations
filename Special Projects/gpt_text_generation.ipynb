{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-attention with causal masking\n",
    "First, implement self-attention block where information is prevented from flowing from future tokens. This is achieved by masking the upper half of the scaled dot product matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    @staticmethod\n",
    "    def causal_attention_mask(n_dest, n_src, dtype):\n",
    "        \"\"\"\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        return tf.cast(m, dtype)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "\n",
    "        # prevent information flow from future tokens\n",
    "        shape = tf.shape(scaled_score)\n",
    "        dim_dest, dim_src = shape[2], shape[3]\n",
    "        attention_mask = self.causal_attention_mask(\n",
    "            dim_dest, dim_src, scaled_score.dtype\n",
    "        )\n",
    "        attention_mask = tf.reshape(attention_mask, [1, 1, dim_dest, dim_src])\n",
    "        scaled_score = scaled_score * attention_mask - 1e4 * (1 - attention_mask)\n",
    "\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Transformer block as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_output = self.att(inputs)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an embedding layer\n",
    "Create two seperate embedding layers: one for tokens and one for token index (positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the miniature GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 100  # Max sequence size\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\", loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for word-level language modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  3958k      0  0:00:20  0:00:20 --:--:-- 9054k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 files\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# The dataset contains each review in a separate text file\n",
    "# The text files are present in four different folders\n",
    "# Create a list all files\n",
    "filenames = []\n",
    "directories = [\n",
    "    \"aclImdb/train/pos\",\n",
    "    \"aclImdb/train/neg\",\n",
    "    \"aclImdb/test/pos\",\n",
    "    \"aclImdb/test/neg\",\n",
    "]\n",
    "for dir in directories:\n",
    "    for f in os.listdir(dir):\n",
    "        filenames.append(os.path.join(dir, f))\n",
    "\n",
    "print(f\"{len(filenames)} files\")\n",
    "\n",
    "# Create a dataset from text files\n",
    "random.shuffle(filenames)\n",
    "text_ds = tf.data.TextLineDataset(filenames)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "    lowercased = tf.strings.lower(input_string)\n",
    "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "# Create a ectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a Keras callback for generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_tokens.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt = \"this movie is\"\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 40\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 43s 26ms/step - loss: 5.5620 - dense_6_loss: 5.5620\n",
      "generated text:\n",
      "this movie is so horrible . it was the first episode , and it was not one of my favorite movies . i 've read all the comments i was very disappointed , and i 've never read that the book was not quite\n",
      "\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 4.5499 - dense_6_loss: 4.5499\n",
      "generated text:\n",
      "this movie is not a big movie i think it has to be a good movie with great plot lines . there 's a lot of fun . the story is not a good movie . but it 's not that good for this\n",
      "\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 4.3268 - dense_6_loss: 4.3268\n",
      "generated text:\n",
      "this movie is so boring you can easily get . this film has a look at the end of the movie , which is very disturbing . the characters are very unrealistic and is very believable . the movie is very realistic . i\n",
      "\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 4.1850 - dense_6_loss: 4.1850\n",
      "generated text:\n",
      "this movie is a complete waste of time . its time it was a great movie . the storyline , the movie starts in a theater when he gets a chance . i saw it for it when it first came out ! and\n",
      "\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 4.0774 - dense_6_loss: 4.0774\n",
      "generated text:\n",
      "this movie is an enjoyable film to watch , with all of them and the other way it should be . it 's not as good as they try and the [UNK] in the movie are so poor . there is no plot ,\n",
      "\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 3.9890 - dense_6_loss: 3.9890\n",
      "generated text:\n",
      "this movie is the only good thing that it 's about it is that it 's one of the most boring films ever made . if you want to know who is a good thing , then you can see it . this movie\n",
      "\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 41s 27ms/step - loss: 3.9161 - dense_6_loss: 3.9161\n",
      "generated text:\n",
      "this movie is about an epidemic known as the [UNK] ' . in a nutshell this movie was the first film i was a little offended by this site . the fact that we have been treated as a whole [UNK] ' character and\n",
      "\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 43s 27ms/step - loss: 3.8539 - dense_6_loss: 3.8539\n",
      "generated text:\n",
      "this movie is great , very well done and very well . very realistic . very realistic , believable acting by the story line , and [UNK] the character of the movie . this movie is very real , and i really liked it\n",
      "\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.7985 - dense_6_loss: 3.7985\n",
      "generated text:\n",
      "this movie is a great one ! ! ! ! ! ! it 's like a classic ! ! ! ! ! ! ! ! ! ! ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha ha !\n",
      "\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 41s 27ms/step - loss: 3.7496 - dense_6_loss: 3.7496\n",
      "generated text:\n",
      "this movie is not a complete contradiction of the monkees or popular stars as an alternative [UNK] . the monkees ' the upper class , and the [UNK] and the result is the monkees or the beatles were given to their own their gifts\n",
      "\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 43s 27ms/step - loss: 3.7070 - dense_6_loss: 3.7070\n",
      "generated text:\n",
      "this movie is very bad , i think i should have been this movie in my life . i am not sure what this movie would be so funny that it is like it . this is a great movie that you should be\n",
      "\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.6689 - dense_6_loss: 3.6689\n",
      "generated text:\n",
      "this movie is very very much , it really is very good and very well done and the acting is very well cast . this movie has a great plot and is very believable . i have seen this movie in my memory is\n",
      "\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.6337 - dense_6_loss: 3.6337\n",
      "generated text:\n",
      "this movie is a great movie . the characters , and that 's what i think about it 's a great deal of fun to watch and laugh , i 'm not too funny , but i have no sense respect for the actors\n",
      "\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.6017 - dense_6_loss: 3.6017\n",
      "generated text:\n",
      "this movie is about a woman who wants to go to the bathroom to see the film . and it 's not funny because it doesn 't know what to be , but he gets a girl , has some sort of like a\n",
      "\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 3.5732 - dense_6_loss: 3.5732\n",
      "generated text:\n",
      "this movie is one of the worst horror films i have ever seen . it was a low budget film , not scary at all . the acting is pretty awful , and the plot sucks the fact that the main character is a\n",
      "\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 41s 26ms/step - loss: 3.5467 - dense_6_loss: 3.5467\n",
      "generated text:\n",
      "this movie is very very funny . the plot is very predictable . and the movie moves along with the pace . the acting is weak and the story is slow and boring . there is no plot at all . . . the\n",
      "\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.5207 - dense_6_loss: 3.5207\n",
      "generated text:\n",
      "this movie is very good . it is a good film that doesn 't make sense at all . i was hoping the same would get better . it would not have been a little more of it .     \n",
      "\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.4979 - dense_6_loss: 3.4979\n",
      "generated text:\n",
      "this movie is so bad it has no logic , it 's really bad . it 's not a bad movie for its time . it also makes the worst movie to be made . there is no point . it 's like listening\n",
      "\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.4777 - dense_6_loss: 3.4777\n",
      "generated text:\n",
      "this movie is a very touching film . it 's a great movie and the actors in the history of this world . it 's a [UNK] , a [UNK] in it , but it also does not have much to offer anything to\n",
      "\n",
      "Epoch 20/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.4565 - dense_6_loss: 3.4565\n",
      "generated text:\n",
      "this movie is a very very bad movie . i think it was bad in my life . i was very disappointed in my way i can 't believe i actually thought it could be . the only one thing i would be happy\n",
      "\n",
      "Epoch 21/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.4384 - dense_6_loss: 3.4384\n",
      "generated text:\n",
      "this movie is so bad that it 's not a bad movie ! i think it 's bad , bad acting ! i don 't get me wrong . it 's a bad thing about this movie ! ! ! ? ? ! ?\n",
      "\n",
      "Epoch 22/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.4213 - dense_6_loss: 3.4213\n",
      "generated text:\n",
      "this movie is a very interesting , the plot has absolutely nothing to do . it is a good movie . the story is very interesting . i had the movie for a lot of people who have done this movie , i really\n",
      "\n",
      "Epoch 23/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.4050 - dense_6_loss: 3.4050\n",
      "generated text:\n",
      "this movie is not bad , but if you can see a lot of fun . i have to watch , this film as much as i have to say , that i was not disappointed , but i think it should have been\n",
      "\n",
      "Epoch 24/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.3892 - dense_6_loss: 3.3892\n",
      "generated text:\n",
      "this movie is an amazing movie . i 've got to say that the plot is so bad . i think my favorite scene is that you can watch it . the movie is just the ending is great too , the acting is\n",
      "\n",
      "Epoch 25/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.3746 - dense_6_loss: 3.3746\n",
      "generated text:\n",
      "this movie is a very low . . . . . .but this movie is a bad piece of work and the [UNK] of the movie . the plot is predictable and laughable . the story line is terrible . . . and all\n",
      "\n",
      "Epoch 26/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.3605 - dense_6_loss: 3.3605\n",
      "generated text:\n",
      "this movie is great . the acting is superb . it is very enjoyable , and the story line is great but it is very good . the story is not the story of the movie you can understand how to see the characters\n",
      "\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.3472 - dense_6_loss: 3.3472\n",
      "generated text:\n",
      "this movie is very funny , well acted , beautifully acted and well scripted , i can see a movie . if you 're a good one . i 'm sorry , but the actors did . it was the best movie i have\n",
      "\n",
      "Epoch 28/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.3341 - dense_6_loss: 3.3341\n",
      "generated text:\n",
      "this movie is one of the funniest movies ever ! it was made in the late 90 's and the worst part of my life . it has nothing to do with that ! if you have to be offended by anyone . a\n",
      "\n",
      "Epoch 29/30\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 3.3232 - dense_6_loss: 3.3232\n",
      "generated text:\n",
      "this movie is very bad . the characters have no redeeming values , it makes no sense , all of the characters is the only one who 's really good and is a bad sign with the bad guys . the bad guy 's\n",
      "\n",
      "Epoch 30/30\n",
      "1563/1563 [==============================] - 43s 28ms/step - loss: 3.3117 - dense_6_loss: 3.3117\n",
      "generated text:\n",
      "this movie is one of my friends and i am currently working on this site . i am a very disappointed that it is the best film ever made . the characters are not the actors , and i don 't care about any\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3adc22f7f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.fit(text_ds, epochs=30, callbacks=[text_gen_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
