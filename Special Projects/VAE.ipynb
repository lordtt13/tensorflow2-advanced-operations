{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import time\n",
    "import glob\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "# Normalizing the images to the range of [0., 1.]\n",
    "train_images /= 255.\n",
    "test_images /= 255.\n",
    "\n",
    "# Binarization\n",
    "train_images[train_images >= .5] = 1.\n",
    "train_images[train_images < .5] = 0.\n",
    "test_images[test_images >= .5] = 1.\n",
    "test_images[test_images < .5] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BUF = 60000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "TEST_BUF = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(TRAIN_BUF).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_images).shuffle(TEST_BUF).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wire up the generative and inference network with tf.keras.Sequential\n",
    "\n",
    "In our VAE example, we use two small ConvNets for the generative and inference network. Since these neural nets are small, we use tf.keras.Sequential to simplify our code.\n",
    "\n",
    "- Generative Network\n",
    "\n",
    "This defines the generative model which takes a latent encoding as input, and outputs the parameters for a conditional distribution of the observation, i.e. . Additionally, we use a unit Gaussian prior  for the latent variable.\n",
    "\n",
    "- Inference Network\n",
    "\n",
    "This defines an approximate posterior distribution , which takes as input an observation and outputs a set of parameters for the conditional distribution of the latent representation. In this example, we simply model this distribution as a diagonal Gaussian. In this case, the inference network outputs the mean and log-variance parameters of a factorized Gaussian (log-variance instead of the variance directly is for numerical stability).\n",
    "\n",
    "- Reparameterization Trick\n",
    "\n",
    "During optimization, we can sample from  by first sampling from a unit Gaussian, and then multiplying by the standard deviation and adding the mean. This ensures the gradients could pass through the sample to the inference network parameters.\n",
    "\n",
    "- Network architecture\n",
    "\n",
    "For the inference network, we use two convolutional layers followed by a fully-connected layer. In the generative network, we mirror this architecture by using a fully-connected layer followed by three convolution transpose layers (a.k.a. deconvolutional layers in some contexts). Note, it's common practice to avoid using batch normalization when training VAEs, since the additional stochasticity due to using mini-batches may aggravate instability on top of the stochasticity from sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(CVAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.inference_net = tf.keras.Sequential(\n",
    "      [\n",
    "          tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "          tf.keras.layers.Conv2D(\n",
    "              filters=32, kernel_size=3, strides=(2, 2), activation = 'relu'),\n",
    "          tf.keras.layers.Conv2D(\n",
    "              filters=64, kernel_size=3, strides=(2, 2), activation = 'relu'),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          # No activation\n",
    "          tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    self.generative_net = tf.keras.Sequential(\n",
    "        [\n",
    "          tf.keras.layers.InputLayer(input_shape = (latent_dim,)),\n",
    "          tf.keras.layers.Dense(units = 7*7*32, activation = tf.nn.relu),\n",
    "          tf.keras.layers.Reshape(target_shape = (7, 7, 32)),\n",
    "          tf.keras.layers.Conv2DTranspose(\n",
    "              filters = 64,\n",
    "              kernel_size = 3,\n",
    "              strides = (2, 2),\n",
    "              padding = \"SAME\",\n",
    "              activation = 'relu'),\n",
    "          tf.keras.layers.Conv2DTranspose(\n",
    "              filters = 32,\n",
    "              kernel_size = 3,\n",
    "              strides = (2, 2),\n",
    "              padding = \"SAME\",\n",
    "              activation = 'relu'),\n",
    "          # No activation\n",
    "          tf.keras.layers.Conv2DTranspose(\n",
    "              filters = 1, kernel_size = 3, strides = (1, 1), padding = \"SAME\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  @tf.function\n",
    "  def sample(self, eps = None):\n",
    "    if eps is None:\n",
    "      eps = tf.random.normal(shape = (100, self.latent_dim))\n",
    "    return self.decode(eps, apply_sigmoid = True)\n",
    "\n",
    "  def encode(self, x):\n",
    "    mean, logvar = tf.split(self.inference_net(x), num_or_size_splits = 2, axis = 1)\n",
    "    return mean, logvar\n",
    "\n",
    "  def reparameterize(self, mean, logvar):\n",
    "    eps = tf.random.normal(shape = mean.shape)\n",
    "    return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "  def decode(self, z, apply_sigmoid = False):\n",
    "    logits = self.generative_net(z)\n",
    "    if apply_sigmoid:\n",
    "      probs = tf.sigmoid(logits)\n",
    "      return probs\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VAE_loss.png'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"VAE_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
