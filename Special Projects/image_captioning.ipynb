{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_captioning",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU-p5b4Bo4rQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yub8LdVpRAl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1b46c34e-27ec-4ec9-e368-173b96a261df"
      },
      "source": [
        "# Download caption annotation files\n",
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                          cache_subdir = os.path.abspath('.'),\n",
        "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                          extract = True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "  os.remove(annotation_zip)\n",
        "\n",
        "# Download image files\n",
        "image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir = os.path.abspath('.'),\n",
        "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract = True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "else:\n",
        "  PATH = os.path.abspath('.') + image_folder"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "252878848/252872794 [==============================] - 20s 0us/step\n",
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510574080/13510573713 [==============================] - 986s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiCAP3DTpZ1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the json file\n",
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Store captions and image names in vectors\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['image_id']\n",
        "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
        "\n",
        "    all_img_name_vector.append(full_coco_image_path)\n",
        "    all_captions.append(caption)\n",
        "\n",
        "# Shuffle captions and image_names together\n",
        "# Set a random state\n",
        "train_captions, img_name_vector = shuffle(all_captions,\n",
        "                                          all_img_name_vector,\n",
        "                                          random_state = 1)\n",
        "\n",
        "# Select the first 30000 captions from the shuffled set\n",
        "num_examples = 30000\n",
        "train_captions = train_captions[:num_examples]\n",
        "img_name_vector = img_name_vector[:num_examples]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty5rJNhfptPU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc99ee1f-ce3e-4aea-85de-2068ef9263cf"
      },
      "source": [
        "len(train_captions), len(all_captions)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 414113)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvE_GlCDvMvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess the images using InceptionV3\n",
        "\n",
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels = 3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2z0R3FHEvXB3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9955fb38-0585-4ff4-d0cd-1133e6363cb0"
      },
      "source": [
        "# Initialize InceptionV3 and load the pretrained Imagenet weights\n",
        "\n",
        "image_model = tf.keras.applications.InceptionV3(include_top = False,\n",
        "                                                weights = 'imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 5s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9LT1oJbvuVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pre-process each image with InceptionV3 and cache the output to disk\n",
        "\n",
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls = tf.data.experimental.AUTOTUNE).batch(16)\n",
        "\n",
        "for img, path in image_dataset:\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw41HsBnwCtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess and tokenize the captions\n",
        "\n",
        "# Find the maximum length of any caption in our dataset\n",
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-VgMI91wJ_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose the top 5000 words from the vocabulary\n",
        "top_k = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = top_k,\n",
        "                                                  oov_token = \"<unk>\",\n",
        "                                                  filters = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0RZdgUpwP-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKMJXVyLwTB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the tokenized vectors\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQCE7lVXx76q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad each vector to the max_length of the captions\n",
        "\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding = 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRoAKam4yDDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training and validation sets using an 80-20 split\n",
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
        "                                                                    cap_vector,\n",
        "                                                                    test_size = 0.2,\n",
        "                                                                    random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9xTT_2zzrX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e3448d3-6691-4165-9b52-1b832e1e1ea4"
      },
      "source": [
        "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 24000, 6000, 6000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKrPeJPvzvfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a tf.data dataset for training\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = top_k + 1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2vYOgqZz8wT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the numpy files\n",
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c83cIMRAz_9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz-CE-Jx0Ft8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # score shape == (batch_size, 64, hidden_size)\n",
        "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    # you get 1 at the last axis because you are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(self.V(score), axis = 1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5hbJduQ0zsa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it using pickle\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QBjqVYJ07rN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer = 'glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4q14tw716R8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-dJC1h02BL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits = True, reduction = 'none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype = loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceU8khaR2HE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "ckpt = tf.train.Checkpoint(encoder = encoder,\n",
        "                           decoder = decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efo-44q92M1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkZDh6cq2Si6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_plot = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZw_vNgg2XxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size = target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  return loss, total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21ThTNoA2xoi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "ee3a2af5-d1a4-49fc-9564-7e338806720d"
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                         total_loss/num_steps))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.6864\n",
            "Epoch 1 Batch 100 Loss 0.6918\n",
            "Epoch 1 Batch 200 Loss 0.7978\n",
            "Epoch 1 Batch 300 Loss 0.7184\n",
            "Epoch 1 Loss 0.701119\n",
            "Time taken for 1 epoch 362.5259337425232 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.6679\n",
            "Epoch 2 Batch 100 Loss 0.6595\n",
            "Epoch 2 Batch 200 Loss 0.6611\n",
            "Epoch 2 Batch 300 Loss 0.6179\n",
            "Epoch 2 Loss 0.661936\n",
            "Time taken for 1 epoch 365.9795916080475 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.6479\n",
            "Epoch 3 Batch 100 Loss 0.6110\n",
            "Epoch 3 Batch 200 Loss 0.6290\n",
            "Epoch 3 Batch 300 Loss 0.6147\n",
            "Epoch 3 Loss 0.628880\n",
            "Time taken for 1 epoch 368.65250182151794 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.6489\n",
            "Epoch 4 Batch 100 Loss 0.5691\n",
            "Epoch 4 Batch 200 Loss 0.6198\n",
            "Epoch 4 Batch 300 Loss 0.6000\n",
            "Epoch 4 Loss 0.598866\n",
            "Time taken for 1 epoch 367.7360863685608 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.5400\n",
            "Epoch 5 Batch 100 Loss 0.5682\n",
            "Epoch 5 Batch 200 Loss 0.6139\n",
            "Epoch 5 Batch 300 Loss 0.5792\n",
            "Epoch 5 Loss 0.568799\n",
            "Time taken for 1 epoch 367.04702639579773 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew0o6jgGBZ0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "1aed4d6b-d011-4a85-e4ed-1aa0d718e7f6"
      },
      "source": [
        "plt.plot(loss_plot)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Plot')\n",
        "plt.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvTlk1XSBe_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Caption\n",
        "\n",
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size = 1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLRoGwKSCyOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention(image, result, attention_plot):\n",
        "    temp_image = np.array(Image.open(image))\n",
        "\n",
        "    fig = plt.figure(figsize = (10, 10))\n",
        "\n",
        "    len_result = len(result)\n",
        "    for l in range(len_result):\n",
        "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
        "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap = 'gray', alpha = 0.6, extent = img.get_extent())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}