{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-02-08 14:10:08,087] Making new env: Pendulum-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirac13/.conda/envs/tf2/lib/python3.8/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v0\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement better exploration by the Actor network, we use noisy perturbations, specifically an Ornstein-Uhlenbeck process for generating noise, as described in the paper. It samples noise from a correlated normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations\n",
    "    # such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy() returns an action sampled from our Actor network plus some noise for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training HyperParams\n",
    "\n",
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement our main training loop, and iterate over episodes. We sample actions using policy() and train with learn() at each time step, along with updating the Target networks at a rate tau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1195.8075027868022\n",
      "Episode * 1 * Avg Reward is ==> -1311.7456888403901\n",
      "Episode * 2 * Avg Reward is ==> -1276.0452239492079\n",
      "Episode * 3 * Avg Reward is ==> -1212.015915510167\n",
      "Episode * 4 * Avg Reward is ==> -1277.524743133216\n",
      "Episode * 5 * Avg Reward is ==> -1297.7631497257235\n",
      "Episode * 6 * Avg Reward is ==> -1338.731030618654\n",
      "Episode * 7 * Avg Reward is ==> -1326.9442563772755\n",
      "Episode * 8 * Avg Reward is ==> -1285.5785590561852\n",
      "Episode * 9 * Avg Reward is ==> -1255.798719956642\n",
      "Episode * 10 * Avg Reward is ==> -1203.4361693167232\n",
      "Episode * 11 * Avg Reward is ==> -1169.9985547452043\n",
      "Episode * 12 * Avg Reward is ==> -1133.9647928745505\n",
      "Episode * 13 * Avg Reward is ==> -1115.252427384631\n",
      "Episode * 14 * Avg Reward is ==> -1067.260789821389\n",
      "Episode * 15 * Avg Reward is ==> -1009.0103311917173\n",
      "Episode * 16 * Avg Reward is ==> -963.8465670722445\n",
      "Episode * 17 * Avg Reward is ==> -917.4239708173162\n",
      "Episode * 18 * Avg Reward is ==> -875.8585739322315\n",
      "Episode * 19 * Avg Reward is ==> -844.7616410723485\n",
      "Episode * 20 * Avg Reward is ==> -810.6892652303958\n",
      "Episode * 21 * Avg Reward is ==> -785.5421546719058\n",
      "Episode * 22 * Avg Reward is ==> -766.4225824563682\n",
      "Episode * 23 * Avg Reward is ==> -755.2290887149599\n",
      "Episode * 24 * Avg Reward is ==> -729.9287237935906\n",
      "Episode * 25 * Avg Reward is ==> -711.1489940828704\n",
      "Episode * 26 * Avg Reward is ==> -693.1482961444368\n",
      "Episode * 27 * Avg Reward is ==> -672.525024399827\n",
      "Episode * 28 * Avg Reward is ==> -653.8651275568607\n",
      "Episode * 29 * Avg Reward is ==> -632.3515547497296\n",
      "Episode * 30 * Avg Reward is ==> -623.9588299831296\n",
      "Episode * 31 * Avg Reward is ==> -604.8516536154516\n",
      "Episode * 32 * Avg Reward is ==> -590.4837582268464\n",
      "Episode * 33 * Avg Reward is ==> -583.5631016909816\n",
      "Episode * 34 * Avg Reward is ==> -574.4690829385094\n",
      "Episode * 35 * Avg Reward is ==> -558.8572540436448\n",
      "Episode * 36 * Avg Reward is ==> -550.2104874566926\n",
      "Episode * 37 * Avg Reward is ==> -539.2279013536736\n",
      "Episode * 38 * Avg Reward is ==> -528.5374537277893\n",
      "Episode * 39 * Avg Reward is ==> -525.0006228776701\n",
      "Episode * 40 * Avg Reward is ==> -498.2815754606421\n",
      "Episode * 41 * Avg Reward is ==> -471.3903319178644\n",
      "Episode * 42 * Avg Reward is ==> -444.4988545862072\n",
      "Episode * 43 * Avg Reward is ==> -425.1502880796734\n",
      "Episode * 44 * Avg Reward is ==> -395.255793395665\n",
      "Episode * 45 * Avg Reward is ==> -379.9330773313435\n",
      "Episode * 46 * Avg Reward is ==> -362.4023128669871\n",
      "Episode * 47 * Avg Reward is ==> -343.81403220539164\n",
      "Episode * 48 * Avg Reward is ==> -329.1916659248098\n",
      "Episode * 49 * Avg Reward is ==> -307.6210508857072\n",
      "Episode * 50 * Avg Reward is ==> -299.3105541587282\n",
      "Episode * 51 * Avg Reward is ==> -279.310907747422\n",
      "Episode * 52 * Avg Reward is ==> -264.9857782376773\n",
      "Episode * 53 * Avg Reward is ==> -246.20865722837024\n",
      "Episode * 54 * Avg Reward is ==> -239.46493867134603\n",
      "Episode * 55 * Avg Reward is ==> -239.1843600115213\n",
      "Episode * 56 * Avg Reward is ==> -236.37731038526826\n",
      "Episode * 57 * Avg Reward is ==> -236.10053536910374\n",
      "Episode * 58 * Avg Reward is ==> -236.08159890877545\n",
      "Episode * 59 * Avg Reward is ==> -232.8647950195018\n",
      "Episode * 60 * Avg Reward is ==> -235.53944864095712\n",
      "Episode * 61 * Avg Reward is ==> -229.1295994145367\n",
      "Episode * 62 * Avg Reward is ==> -229.36419517050325\n",
      "Episode * 63 * Avg Reward is ==> -219.98477637872082\n",
      "Episode * 64 * Avg Reward is ==> -220.0524765355889\n",
      "Episode * 65 * Avg Reward is ==> -217.05573294277988\n",
      "Episode * 66 * Avg Reward is ==> -214.5425614779589\n",
      "Episode * 67 * Avg Reward is ==> -214.68352570786828\n",
      "Episode * 68 * Avg Reward is ==> -211.4344381867332\n",
      "Episode * 69 * Avg Reward is ==> -217.0927656253199\n",
      "Episode * 70 * Avg Reward is ==> -213.9892961721211\n",
      "Episode * 71 * Avg Reward is ==> -213.77724712384367\n",
      "Episode * 72 * Avg Reward is ==> -216.80626918716007\n",
      "Episode * 73 * Avg Reward is ==> -213.75868501142628\n",
      "Episode * 74 * Avg Reward is ==> -210.30745208784174\n",
      "Episode * 75 * Avg Reward is ==> -219.0873759976083\n",
      "Episode * 76 * Avg Reward is ==> -216.21163181537818\n",
      "Episode * 77 * Avg Reward is ==> -216.1033501247216\n",
      "Episode * 78 * Avg Reward is ==> -219.10878899485334\n",
      "Episode * 79 * Avg Reward is ==> -212.5215170072015\n",
      "Episode * 80 * Avg Reward is ==> -209.68513226595138\n",
      "Episode * 81 * Avg Reward is ==> -206.9118120715394\n",
      "Episode * 82 * Avg Reward is ==> -207.05866094700704\n",
      "Episode * 83 * Avg Reward is ==> -204.10545892344624\n",
      "Episode * 84 * Avg Reward is ==> -198.64922508994363\n",
      "Episode * 85 * Avg Reward is ==> -185.52065341099834\n",
      "Episode * 86 * Avg Reward is ==> -166.5935295050881\n",
      "Episode * 87 * Avg Reward is ==> -157.21758903438172\n",
      "Episode * 88 * Avg Reward is ==> -151.1986716139388\n",
      "Episode * 89 * Avg Reward is ==> -151.24170873419848\n",
      "Episode * 90 * Avg Reward is ==> -142.82661455799882\n",
      "Episode * 91 * Avg Reward is ==> -149.36392103576236\n",
      "Episode * 92 * Avg Reward is ==> -149.26484393851797\n",
      "Episode * 93 * Avg Reward is ==> -146.47750137239194\n",
      "Episode * 94 * Avg Reward is ==> -146.45340984805185\n",
      "Episode * 95 * Avg Reward is ==> -146.46061950269979\n",
      "Episode * 96 * Avg Reward is ==> -146.4227500212631\n",
      "Episode * 97 * Avg Reward is ==> -143.52264083923032\n",
      "Episode * 98 * Avg Reward is ==> -143.34748684949665\n",
      "Episode * 99 * Avg Reward is ==> -143.3571312861835\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
