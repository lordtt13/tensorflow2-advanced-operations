{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 16.1181 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "input_shape = [20]\n",
    "x_train = np.random.randn(1, 20).astype(np.float32)\n",
    "y_train = tf.keras.utils.to_categorical(np.random.randn(1), num_classes = 20)\n",
    "\n",
    "def setup_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(20, input_shape = input_shape),\n",
    "      tf.keras.layers.Flatten()\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "def setup_pretrained_weights():\n",
    "  model = setup_model()\n",
    "\n",
    "  model.compile(\n",
    "      loss = tf.keras.losses.categorical_crossentropy,\n",
    "      optimizer = 'adam',\n",
    "      metrics = ['accuracy']\n",
    "  )\n",
    "\n",
    "  model.fit(x_train, y_train)\n",
    "\n",
    "  _, pretrained_weights = tempfile.mkstemp('.tf')\n",
    "\n",
    "  model.save_weights(pretrained_weights)\n",
    "\n",
    "  return pretrained_weights\n",
    "\n",
    "def get_gzipped_model_size(model):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "  _, keras_file = tempfile.mkstemp('.h5')\n",
    "  model.save(keras_file, include_optimizer = False)\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression = zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(keras_file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)\n",
    "\n",
    "setup_model()\n",
    "pretrained_weights = setup_pretrained_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune whole model (Sequential and Functional)\n",
    "\n",
    "Tips for better model accuracy:\n",
    "- Try \"Prune some layers\" to skip pruning the layers that reduce accuracy the most.\n",
    "- It's generally better to finetune with pruning as opposed to training from scratch.\n",
    "\n",
    "To make the whole model train with pruning, apply tfmot.sparsity.keras.prune_low_magnitude to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tanmay/anaconda3/lib/python3.7/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:199: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_dense_2  (None, 20)                822       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_flatten_ (None, 20)                1         \n",
      "=================================================================\n",
      "Total params: 823\n",
      "Trainable params: 420\n",
      "Non-trainable params: 403\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = setup_model()\n",
    "base_model.load_weights(pretrained_weights) # optional but recommended.\n",
    "\n",
    "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(base_model)\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune some layers (Sequential and Functional)\n",
    "\n",
    "Pruning a model can have a negative effect on accuracy. You can selectively prune layers of a model to explore the trade-off between accuracy, speed, and model size.\n",
    "\n",
    "Tips for better model accuracy:\n",
    "- It's generally better to finetune with pruning as opposed to training from scratch.\n",
    "- Try pruning the later layers instead of the first layers.\n",
    "- Avoid pruning critical layers (e.g. attention mechanism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_dense_3  (None, 20)                822       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 822\n",
      "Trainable params: 420\n",
      "Non-trainable params: 402\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a base model\n",
    "base_model = setup_model()\n",
    "base_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n",
    "\n",
    "# Helper function uses `prune_low_magnitude` to make only the \n",
    "# Dense layers train with pruning.\n",
    "def apply_pruning_to_dense(layer):\n",
    "  if isinstance(layer, tf.keras.layers.Dense):\n",
    "    return tfmot.sparsity.keras.prune_low_magnitude(layer)\n",
    "  return layer\n",
    "\n",
    "# Use `tf.keras.models.clone_model` to apply `apply_pruning_to_dense` \n",
    "# to the layers of the model.\n",
    "model_for_pruning = tf.keras.models.clone_model(\n",
    "    base_model,\n",
    "    clone_function = apply_pruning_to_dense,\n",
    ")\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_4  (None, 10)                412       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 412\n",
      "Trainable params: 210\n",
      "Non-trainable params: 202\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Functional Example\n",
    "\n",
    "i = tf.keras.Input(shape = (20,))\n",
    "x = tfmot.sparsity.keras.prune_low_magnitude(tf.keras.layers.Dense(10))(i)\n",
    "o = tf.keras.layers.Flatten()(x)\n",
    "model_for_pruning = tf.keras.Model(inputs = i, outputs = o)\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_dense_5  (None, 20)                822       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 822\n",
      "Trainable params: 420\n",
      "Non-trainable params: 402\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential Example\n",
    "\n",
    "model_for_pruning = tf.keras.Sequential([\n",
    "  tfmot.sparsity.keras.prune_low_magnitude(tf.keras.layers.Dense(20, input_shape = input_shape)),\n",
    "  tf.keras.layers.Flatten()\n",
    "])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prune custom Keras layer or modify parts of layer to prune\n",
    "\n",
    "Common mistake: pruning the bias usually harms model accuracy too much.\n",
    "\n",
    "tfmot.sparsity.keras.PrunableLayer serves two use cases:\n",
    "- Prune a custom Keras layer\n",
    "- Modify parts of a built-in Keras layer to prune.\n",
    "\n",
    "For an example, the API defaults to only pruning the kernel of the Dense layer. The example below prunes the bias also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_my_dense (None, 20)                843       \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 843\n",
      "Trainable params: 420\n",
      "Non-trainable params: 423\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Dense, tfmot.sparsity.keras.PrunableLayer):\n",
    "\n",
    "  def get_prunable_weights(self):\n",
    "    # Prune bias also, though that usually harms model accuracy too much.\n",
    "    return [self.kernel, self.bias]\n",
    "\n",
    "# Use `prune_low_magnitude` to make the `MyDenseLayer` layer train with pruning.\n",
    "model_for_pruning = tf.keras.Sequential([\n",
    "  tfmot.sparsity.keras.prune_low_magnitude(MyDenseLayer(20, input_shape = input_shape)),\n",
    "  tf.keras.layers.Flatten()\n",
    "])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 16.1181 - accuracy: 0.0000e+00\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 16.1181 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9cc85ce350>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = setup_model()\n",
    "base_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n",
    "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(base_model)\n",
    "\n",
    "log_dir = tempfile.mkdtemp()\n",
    "callbacks = [\n",
    "    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    # Log sparsity and other metrics in Tensorboard.\n",
    "    tfmot.sparsity.keras.PruningSummaries(log_dir = log_dir)\n",
    "]\n",
    "\n",
    "model_for_pruning.compile(\n",
    "      loss = tf.keras.losses.categorical_crossentropy,\n",
    "      optimizer = 'adam',\n",
    "      metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model_for_pruning.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    callbacks = callbacks,\n",
    "    epochs = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model.\n",
    "base_model = setup_model()\n",
    "base_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n",
    "model_for_pruning = tfmot.sparsity.keras.prune_low_magnitude(base_model)\n",
    "\n",
    "# Boilerplate\n",
    "loss = tf.keras.losses.categorical_crossentropy\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "log_dir = tempfile.mkdtemp()\n",
    "unused_arg = -1\n",
    "epochs = 2\n",
    "batches = 1 # example is hardcoded so that the number of batches cannot change.\n",
    "\n",
    "# Non-boilerplate.\n",
    "model_for_pruning.optimizer = optimizer\n",
    "step_callback = tfmot.sparsity.keras.UpdatePruningStep()\n",
    "step_callback.set_model(model_for_pruning)\n",
    "log_callback = tfmot.sparsity.keras.PruningSummaries(log_dir = log_dir) # Log sparsity and other metrics in Tensorboard.\n",
    "log_callback.set_model(model_for_pruning)\n",
    "\n",
    "step_callback.on_train_begin() # run pruning callback\n",
    "for _ in range(epochs):\n",
    "  log_callback.on_epoch_begin(epoch = unused_arg) # run pruning callback\n",
    "  for _ in range(batches):\n",
    "    step_callback.on_train_batch_begin(batch=unused_arg) # run pruning callback\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = model_for_pruning(x_train, training = True)\n",
    "      loss_value = loss(y_train, logits)\n",
    "      grads = tape.gradient(loss_value, model_for_pruning.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(grads, model_for_pruning.trainable_variables))\n",
    "\n",
    "  step_callback.on_epoch_end(batch = unused_arg) # run pruning callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
